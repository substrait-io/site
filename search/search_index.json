{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Substrait \u00b6 Project Vision \u00b6 Create a well-defined, cross-language specification for data compute operations. This includes a declaration of common operations, custom operations and one or more serialized representations of this specification. The spec focuses on the semantics of each operation and a consistent way to describe. In many ways, the goal of this project is similar to that of the Apache Arrow project. Arrow is focused on a standardized memory representation of columnar data. Substrait is focused on what should be done to data. Example Use Cases \u00b6 Communicate a compute plan between a SQL parser and an execution engine (e.g. Calcite SQL parsing to Arrow C++ compute kernel) Serialize a plan that represents a view for consumption in multiple systems (e.g. Iceberg views in Spark and Trino) Create an alternative plan generation implementation that can connect an existing end-user compute expression system to an existing end-user processing engine (e.g. Pandas operations executed inside SingleStore) Build a pluggable plan visualization tool (e.g. D3 based plan visualizer) Community Principles \u00b6 Be inclusive and open to all. If you want to join the project, open a PR or issue or join the Slack channel (TBD) Ensure a diverse set of contributors that come from multiple data backgrounds to maximize general utility. Build a specification based on open consensus. Make the specification and all tools freely available on a permissive license (ApacheV2)","title":"Substrait"},{"location":"#substrait","text":"","title":"Substrait"},{"location":"#project-vision","text":"Create a well-defined, cross-language specification for data compute operations. This includes a declaration of common operations, custom operations and one or more serialized representations of this specification. The spec focuses on the semantics of each operation and a consistent way to describe. In many ways, the goal of this project is similar to that of the Apache Arrow project. Arrow is focused on a standardized memory representation of columnar data. Substrait is focused on what should be done to data.","title":"Project Vision"},{"location":"#example-use-cases","text":"Communicate a compute plan between a SQL parser and an execution engine (e.g. Calcite SQL parsing to Arrow C++ compute kernel) Serialize a plan that represents a view for consumption in multiple systems (e.g. Iceberg views in Spark and Trino) Create an alternative plan generation implementation that can connect an existing end-user compute expression system to an existing end-user processing engine (e.g. Pandas operations executed inside SingleStore) Build a pluggable plan visualization tool (e.g. D3 based plan visualizer)","title":"Example Use Cases"},{"location":"#community-principles","text":"Be inclusive and open to all. If you want to join the project, open a PR or issue or join the Slack channel (TBD) Ensure a diverse set of contributors that come from multiple data backgrounds to maximize general utility. Build a specification based on open consensus. Make the specification and all tools freely available on a permissive license (ApacheV2)","title":"Community Principles"},{"location":"community/","text":"Community \u00b6 Substrait is developed as a consensus-driven open source product under the Apache 2.0 license. Development is done in the open leveraging GitHub issues and PRs. Get In Touch \u00b6 GitHub Issues Substrait is developed via GitHub issues and pull requests. If you see a problem or want to enhance the product, we suggest you file a GitHub issue for developers to review. Twitter The @substrait_io account on Twitter is our official account. Follow-up to keep to date on what is happening with Substrait! Docs Our website is all maintained in our source repository. If there is something you think can be improved, feel free to fork our repository and post a pull request. Reviewable We use Reviewable (in addition to GitHub reviews) to collaborate on code and docs. They are a great service and support OSS projects. Contribution \u00b6 All contributors are welcome to Substrait.","title":"Community"},{"location":"community/#community","text":"Substrait is developed as a consensus-driven open source product under the Apache 2.0 license. Development is done in the open leveraging GitHub issues and PRs.","title":"Community"},{"location":"community/#get-in-touch","text":"GitHub Issues Substrait is developed via GitHub issues and pull requests. If you see a problem or want to enhance the product, we suggest you file a GitHub issue for developers to review. Twitter The @substrait_io account on Twitter is our official account. Follow-up to keep to date on what is happening with Substrait! Docs Our website is all maintained in our source repository. If there is something you think can be improved, feel free to fork our repository and post a pull request. Reviewable We use Reviewable (in addition to GitHub reviews) to collaborate on code and docs. They are a great service and support OSS projects.","title":"Get In Touch"},{"location":"community/#contribution","text":"All contributors are welcome to Substrait.","title":"Contribution"},{"location":"expressions/aggregate_functions/","text":"Aggregate Functions \u00b6 Aggregate functions are functions that define an operation which consumes values from multiple records to a produce a single output. Aggregate functions in SQL are typically used in GROUP BY functions. Aggregate functions are similar to scalar functions and function signatures with a small set of different properties. Aggregate function signatures contain all of the properties defined for scalar functions . Additionally, they contain the properties below Property Description Required Inherits All properties defined for scalar function N/A Ordered Whether this aggregation function should allow user ordering Optional, defaults to false Maximum set size Maximum allowed set size as an unsigned integer Optional, defaults to unlimited Decomposable Whether the funtion can be executed in one or more intermediate steps. Valid options are: NONE, ONE, MANY, describing how intermediate steps can be taken. Optional, defaults to NONE Intermediate Output Type If the function is decomposable, represents the intermediate output type that is used, if the function is defined as either ONE or MANY decomposable. Will be a struct in many cases. Required for ONE and MANY. Aggregate Binding \u00b6 When Binding an Aggregate function, the binding must include the following additional properties beyond the standard scalar binding properties: Property Description Phase Describes the input type of the data: [INITIAL_TO_INTERMEDIATE, INTERMEDIATE_TO_INTERMEDIATE, INITIAL_TO_RESULT, INTERMEDIATE_TO_RESULT] describing what portion of the operation is required. For functions that are NOT decomposable, the only valid option will be INITIAL_TO_RESULT. Ordering One or more ordering keys along with key order (ASC|DESC|NULL FIRST, etc), declared similar to the sort keys in a order by relational operation. Only allowed in cases where the function signature supports Ordering.","title":"Aggregate Functions"},{"location":"expressions/aggregate_functions/#aggregate-functions","text":"Aggregate functions are functions that define an operation which consumes values from multiple records to a produce a single output. Aggregate functions in SQL are typically used in GROUP BY functions. Aggregate functions are similar to scalar functions and function signatures with a small set of different properties. Aggregate function signatures contain all of the properties defined for scalar functions . Additionally, they contain the properties below Property Description Required Inherits All properties defined for scalar function N/A Ordered Whether this aggregation function should allow user ordering Optional, defaults to false Maximum set size Maximum allowed set size as an unsigned integer Optional, defaults to unlimited Decomposable Whether the funtion can be executed in one or more intermediate steps. Valid options are: NONE, ONE, MANY, describing how intermediate steps can be taken. Optional, defaults to NONE Intermediate Output Type If the function is decomposable, represents the intermediate output type that is used, if the function is defined as either ONE or MANY decomposable. Will be a struct in many cases. Required for ONE and MANY.","title":"Aggregate Functions"},{"location":"expressions/aggregate_functions/#aggregate-binding","text":"When Binding an Aggregate function, the binding must include the following additional properties beyond the standard scalar binding properties: Property Description Phase Describes the input type of the data: [INITIAL_TO_INTERMEDIATE, INTERMEDIATE_TO_INTERMEDIATE, INITIAL_TO_RESULT, INTERMEDIATE_TO_RESULT] describing what portion of the operation is required. For functions that are NOT decomposable, the only valid option will be INITIAL_TO_RESULT. Ordering One or more ordering keys along with key order (ASC|DESC|NULL FIRST, etc), declared similar to the sort keys in a order by relational operation. Only allowed in cases where the function signature supports Ordering.","title":"Aggregate Binding"},{"location":"expressions/embedded_functions/","text":"Embedded Functions \u00b6 Embedded functions are a special kind of function where the implementation is embedded within the actual plan. They are commonly used in tools where a user intersperses business logic within a data pipeline. This is more common in data science workflows than traditional SQL workflows. Embedded functions are not pre-registered. Embedded functions require that data be consumed and produced with a standard API, may require memory allocation and have determinate error reporting behavior. They may also have specific runtime dependencies. For example, a python pickle function may depend on pyarrow 5.0 and pynessie 1.0. Properties for an embedded function include: Property Description Required Function Type The type of embedded function presented Required Function Properties Function properties, one of those items defined below. Required Output Type The fully resolved output type for this embedded function. Required Function Details \u00b6 There are many type of types of possible stored functions. For each, Substrait works to expose the function in as descriptive a way as possible to support the largest number of consumers. Python Pickle Function Type \u00b6 Property Description Required Pickle Body binary pickle encoded function using [TBD] api representation to access arguments. True Prereqs A list of specific python conda packages that are prerequisites for access (a structured version of a requirements.txt file) Optional, defaults to none WebAssembly Function Type \u00b6 Property Description Required Script WebAssembly function True Prereqs A list of AssemblyScript prerequisites required to compile the assemblyscript function using NPM coordinates Optional, defaults to none Discussion Points \u00b6 What are the common embedded function formats? How do we expose the data for a function? How do we express batching capabilities? How do we ensure/declare containerization?","title":"Embedded Functions"},{"location":"expressions/embedded_functions/#embedded-functions","text":"Embedded functions are a special kind of function where the implementation is embedded within the actual plan. They are commonly used in tools where a user intersperses business logic within a data pipeline. This is more common in data science workflows than traditional SQL workflows. Embedded functions are not pre-registered. Embedded functions require that data be consumed and produced with a standard API, may require memory allocation and have determinate error reporting behavior. They may also have specific runtime dependencies. For example, a python pickle function may depend on pyarrow 5.0 and pynessie 1.0. Properties for an embedded function include: Property Description Required Function Type The type of embedded function presented Required Function Properties Function properties, one of those items defined below. Required Output Type The fully resolved output type for this embedded function. Required","title":"Embedded Functions"},{"location":"expressions/embedded_functions/#function-details","text":"There are many type of types of possible stored functions. For each, Substrait works to expose the function in as descriptive a way as possible to support the largest number of consumers.","title":"Function Details"},{"location":"expressions/embedded_functions/#python-pickle-function-type","text":"Property Description Required Pickle Body binary pickle encoded function using [TBD] api representation to access arguments. True Prereqs A list of specific python conda packages that are prerequisites for access (a structured version of a requirements.txt file) Optional, defaults to none","title":"Python Pickle Function Type"},{"location":"expressions/embedded_functions/#webassembly-function-type","text":"Property Description Required Script WebAssembly function True Prereqs A list of AssemblyScript prerequisites required to compile the assemblyscript function using NPM coordinates Optional, defaults to none","title":"WebAssembly Function Type"},{"location":"expressions/embedded_functions/#discussion-points","text":"What are the common embedded function formats? How do we expose the data for a function? How do we express batching capabilities? How do we ensure/declare containerization?","title":"Discussion Points"},{"location":"expressions/field_references/","text":"Field References \u00b6 In Substrait, all fields are dealt with on a positional basis. Field names are only used at the edge of a plan for the purposes of naming field ids. Each operation returns a simple or compound data type. Additional operations can refer to data within that initial operation using field references. To reference a field, you use a reference based on the type of field position you want to reference. Reference Type Properties Type Applicability Type return Struct Field Ordinal position. Zero-based. Only legal within the range of possible fields within a struct. Selecting an ordinal outside the applicable field range results in an invalid plan. struct Type of field referenced Array Value Array offset. Zero-based. Negative number can be used to describe a offset relative to the end of the array. For example, -1 means the last element in an array. Negative and positive overflows return null values (no wrapping). list type of list Array Slice Array offset and element count. Zero-based. Negative number can be used to describe a offset relative to the end of the array. For example, -1 means the last element in an array. Position does not wrap, nor does length. list Same type as original list Map Key A map value that is matched exactly against available map keys and returned. [TBD, can multiple matches occur?] map Value type of map Map KeyExpression A wildcard string that is matched against a simplified form of regular expressions. Requires the key type of the map to be a character type. [Format detail needed, intention to include basic regex concepts such as greedy/non greedy.] map List of map value type Masked Complex Expression An expression that provides a mask over a schema declaring which portions of the schema should be presented. This allows a user to select a portion of a complex object but mask certain subsections of that same object. any any Compound References \u00b6 References are typically constructed as a sequence. For example: [struct position 0, struct position 1, array offset 2, array slice 1..3]. Validation \u00b6 References must validate against the schema of the record being referenced. If not, an error is expected. Masked Complex Expression \u00b6 A masked complex expression is used to do a subselect of a portion of a complex record. It allows a user to specify the portion of the complex object to consume. Imagine you have a schema of (note that structs are lists fields here, as they are in general in Substrait as field names are not used internally in Substrait): struct: - struct: - integer - list: struct: - i32 - string - string - i32 - i16 - i32 - i64 Given this schema, you could declare a mask in pseudo code such as: 0:[0,1:[..5:[0,2]]],2,3 OR O: - 0 - 1: ..5: -0 -2 1 This mask states that we would like to include fields 0 2 and 3 at the top-level. Within field 0, we want to include subfields 0 and 1. For subfield 0.1, we want to include up to only the first 5 records in the array and only includes fields 0 and 2 within the struct within that array. The resulting schema would be: struct: - struct: - integer - list: struct: - i32 - string - i32 Unwrapping Behavior \u00b6 By default, when only a single field is selected from a struct, that struct is removed. When only a single element is removed from a list, the list is removed. A user can also configure the mask to avoid unwrapping in these cases. [TBD how we express this in the serialization formats.]","title":"Field References"},{"location":"expressions/field_references/#field-references","text":"In Substrait, all fields are dealt with on a positional basis. Field names are only used at the edge of a plan for the purposes of naming field ids. Each operation returns a simple or compound data type. Additional operations can refer to data within that initial operation using field references. To reference a field, you use a reference based on the type of field position you want to reference. Reference Type Properties Type Applicability Type return Struct Field Ordinal position. Zero-based. Only legal within the range of possible fields within a struct. Selecting an ordinal outside the applicable field range results in an invalid plan. struct Type of field referenced Array Value Array offset. Zero-based. Negative number can be used to describe a offset relative to the end of the array. For example, -1 means the last element in an array. Negative and positive overflows return null values (no wrapping). list type of list Array Slice Array offset and element count. Zero-based. Negative number can be used to describe a offset relative to the end of the array. For example, -1 means the last element in an array. Position does not wrap, nor does length. list Same type as original list Map Key A map value that is matched exactly against available map keys and returned. [TBD, can multiple matches occur?] map Value type of map Map KeyExpression A wildcard string that is matched against a simplified form of regular expressions. Requires the key type of the map to be a character type. [Format detail needed, intention to include basic regex concepts such as greedy/non greedy.] map List of map value type Masked Complex Expression An expression that provides a mask over a schema declaring which portions of the schema should be presented. This allows a user to select a portion of a complex object but mask certain subsections of that same object. any any","title":"Field References"},{"location":"expressions/field_references/#compound-references","text":"References are typically constructed as a sequence. For example: [struct position 0, struct position 1, array offset 2, array slice 1..3].","title":"Compound References"},{"location":"expressions/field_references/#validation","text":"References must validate against the schema of the record being referenced. If not, an error is expected.","title":"Validation"},{"location":"expressions/field_references/#masked-complex-expression","text":"A masked complex expression is used to do a subselect of a portion of a complex record. It allows a user to specify the portion of the complex object to consume. Imagine you have a schema of (note that structs are lists fields here, as they are in general in Substrait as field names are not used internally in Substrait): struct: - struct: - integer - list: struct: - i32 - string - string - i32 - i16 - i32 - i64 Given this schema, you could declare a mask in pseudo code such as: 0:[0,1:[..5:[0,2]]],2,3 OR O: - 0 - 1: ..5: -0 -2 1 This mask states that we would like to include fields 0 2 and 3 at the top-level. Within field 0, we want to include subfields 0 and 1. For subfield 0.1, we want to include up to only the first 5 records in the array and only includes fields 0 and 2 within the struct within that array. The resulting schema would be: struct: - struct: - integer - list: struct: - i32 - string - i32","title":"Masked Complex Expression"},{"location":"expressions/field_references/#unwrapping-behavior","text":"By default, when only a single field is selected from a struct, that struct is removed. When only a single element is removed from a list, the list is removed. A user can also configure the mask to avoid unwrapping in these cases. [TBD how we express this in the serialization formats.]","title":"Unwrapping Behavior"},{"location":"expressions/scalar_functions/","text":"Scalar Functions \u00b6 Scalar functions are a function that takes in values from a single record and produces an output value. To clearly specify the definition of functions, Substrait declares a extensible specification plus binding approach to function resolution. This ensures a clear definition of function behavior separate from a particular use of each function while avoiding operations Substrait supports a number of functions. A scalar function signature includes the following properties: Property Description Required Name One or more user friendly utf8 strings that are used to reference this function in languages At least one value is required. List of arguments Argument properties are defined below Optional, defaults to niladic. Determinate Whether this function is expected to reproduce the same output when it is invoked multiple times with the same input. This informs a plan consumer on whether it can constant reduce the defined function. An example would be a random() function, which is typically expected to be evaluated repeatedly despite having the same set of inputs. Optional, defaults to true. Session Dependent Whether this function is influenced by the session context it is invoked within. For example, a function may be influenced by a user who is invoking the function, the time zone of a session, or some other non-obvious parameter. This can inform caching systems on whether a particular function is cacheable. Optional, defaults to false. Variadic Behavior Whether the last argument of the function is variadic. Options include: [Single argument, M..N]. M must be less than or equal to N. N can be defined or left as \u201cunlimited\u201d. Optional, defaults to single value. Description Additional description of function for implementers or users. Should be written human readable to allow exposure to end users. Presented as a map with language => description mappings. E.g. { \"en\": \"This adds two numbers together.\", \"fr\": \"cela ajoute deux nombres\"} . Optional Output Type The output type is expected to be a physical type of the expression. There are two categories of output type: Direct: Output type is declared Complex: Behavior definitions are either declared within the specification or embedded in the signature using [TBD]. Required Implementation Map A map of implementation locations for one or more implementations of the given function. Each key is a function implementation type. Implementation types include examples such as: AthenaArrowLambda, TrinoV361Jar, ArrowCppKernelEnum, GandivaEnum, LinkedIn Transport Jar, etc. [Definition TBD]. Implementation type has one or more properties associated with retrieval of that implementation. Optional Argument Properties \u00b6 Property Description Physical Type The physical types this argument requires. Physical types are used here so that different binds are used when working in a physical plan depending on the execution engines capabilities. In common cases, the system default representation is used. Constant Whether this argument is required to be a constant for invocation. For example, in some system a regular expression pattern would only be accepted as a literal and not a column value reference. Function Ids \u00b6 Each function signature is categorized based on a function signature id. The identifier includes two components: Property Description Organization Id An unsigned 32 bit integer mapped to a list of known organizations listed in the Substrait repository . Function Id An unsigned 32 bit integer mapped to a list of known functions for the specific organization. For the Substrait organization, the function is listed in here . An organization is responsible for managing their own lists of functions. A function id should be used forever. If a function is deleted for any reason, the id should not be reused. Ideally each function signature will have one or more Scalar Function Bindings \u00b6 For scalar functions (one value produced for each record), a function binding declares the function identifier to be used and the input arguments. Key Discussion Points \u00b6 Currently avoiding binary operator concepts (e.g. +) How to define complex behavior definitions? Start with only allowing definition in Substrait? Come up with some kind of way to express output type derivation using something language agnostic (e.g. a WebAssembly scripting language)? Should we have a function version for each function signature? What happens if someone redefines or changes the semantics of a function?","title":"Scalar Functions"},{"location":"expressions/scalar_functions/#scalar-functions","text":"Scalar functions are a function that takes in values from a single record and produces an output value. To clearly specify the definition of functions, Substrait declares a extensible specification plus binding approach to function resolution. This ensures a clear definition of function behavior separate from a particular use of each function while avoiding operations Substrait supports a number of functions. A scalar function signature includes the following properties: Property Description Required Name One or more user friendly utf8 strings that are used to reference this function in languages At least one value is required. List of arguments Argument properties are defined below Optional, defaults to niladic. Determinate Whether this function is expected to reproduce the same output when it is invoked multiple times with the same input. This informs a plan consumer on whether it can constant reduce the defined function. An example would be a random() function, which is typically expected to be evaluated repeatedly despite having the same set of inputs. Optional, defaults to true. Session Dependent Whether this function is influenced by the session context it is invoked within. For example, a function may be influenced by a user who is invoking the function, the time zone of a session, or some other non-obvious parameter. This can inform caching systems on whether a particular function is cacheable. Optional, defaults to false. Variadic Behavior Whether the last argument of the function is variadic. Options include: [Single argument, M..N]. M must be less than or equal to N. N can be defined or left as \u201cunlimited\u201d. Optional, defaults to single value. Description Additional description of function for implementers or users. Should be written human readable to allow exposure to end users. Presented as a map with language => description mappings. E.g. { \"en\": \"This adds two numbers together.\", \"fr\": \"cela ajoute deux nombres\"} . Optional Output Type The output type is expected to be a physical type of the expression. There are two categories of output type: Direct: Output type is declared Complex: Behavior definitions are either declared within the specification or embedded in the signature using [TBD]. Required Implementation Map A map of implementation locations for one or more implementations of the given function. Each key is a function implementation type. Implementation types include examples such as: AthenaArrowLambda, TrinoV361Jar, ArrowCppKernelEnum, GandivaEnum, LinkedIn Transport Jar, etc. [Definition TBD]. Implementation type has one or more properties associated with retrieval of that implementation. Optional","title":"Scalar Functions"},{"location":"expressions/scalar_functions/#argument-properties","text":"Property Description Physical Type The physical types this argument requires. Physical types are used here so that different binds are used when working in a physical plan depending on the execution engines capabilities. In common cases, the system default representation is used. Constant Whether this argument is required to be a constant for invocation. For example, in some system a regular expression pattern would only be accepted as a literal and not a column value reference.","title":"Argument Properties"},{"location":"expressions/scalar_functions/#function-ids","text":"Each function signature is categorized based on a function signature id. The identifier includes two components: Property Description Organization Id An unsigned 32 bit integer mapped to a list of known organizations listed in the Substrait repository . Function Id An unsigned 32 bit integer mapped to a list of known functions for the specific organization. For the Substrait organization, the function is listed in here . An organization is responsible for managing their own lists of functions. A function id should be used forever. If a function is deleted for any reason, the id should not be reused. Ideally each function signature will have one or more","title":"Function Ids"},{"location":"expressions/scalar_functions/#scalar-function-bindings","text":"For scalar functions (one value produced for each record), a function binding declares the function identifier to be used and the input arguments.","title":"Scalar Function Bindings"},{"location":"expressions/scalar_functions/#key-discussion-points","text":"Currently avoiding binary operator concepts (e.g. +) How to define complex behavior definitions? Start with only allowing definition in Substrait? Come up with some kind of way to express output type derivation using something language agnostic (e.g. a WebAssembly scripting language)? Should we have a function version for each function signature? What happens if someone redefines or changes the semantics of a function?","title":"Key Discussion Points"},{"location":"expressions/specialized_record_expressions/","text":"Specialized Record Expressions \u00b6 While most all types of operations could be reduced to functions, in some cases this would be overly simplistic. Instead, it is helpful to construct some other expression constructs. These constructs should be focused on different expression types as opposed to something that directly related to syntantic sugar. For example, CAST and EXTRACT or SQL operations that are presented using specialized syntax. However, they can easily modeled using a function paradigm with minimal complexity. Literal Expressions \u00b6 For each data type, it is possible to create a literal value for that data type. The representation depends on the serialization format. If Expression \u00b6 An if value expression is an expression composed of one if clause, zero or more else if clauses and an else clause. In pseudo code, they are envisioned as: if <boolean expression> then <result expression 1> else if <boolean expression> then <result expression 2> (zero or more times) else <result expression 3> When an if expression is declared, all return expressions must be the same identical type. Shortcut Behavior \u00b6 An if expression is expected to logically short-cicuit on a postive outcome. This means that a skipped else/elseif expression cannot cause an error. For example, this should not actually throw an error despite the fact that the cast operation should fail. if 'value' = 'value' then 0 else cast('hello' as integer) Switch Expression \u00b6 Switch expression allow a selection of alternate branches based on the value of a given expression. They are an optimized form of a generic if expression where all conditions are equality to the same value. In pseudo-code: switch(value) <value 1> => <return 1> (1 or more times) <else> => <return 3> Return values for a switch expression must all be of identical type. Shortcut Behavior \u00b6 As in if expressions, switch expression evaluation should not be interrupted by \u201croads not taken\u201d. Or List Equality Expression \u00b6 A specialized structure that is often used is a large list of possible values. In SQL, these are typically large IN lists. They can composed of one or more fields. There are two common patterns, single value and multi value. In pseudo code they are represented as: Single Value: expression, [<value1>, <value2>, ... <valueN>] Multi Value: [expressionA, expressionB], [[value1a, value1b], [value2a, value2b].. [valueNa, valueNb]] For single value expressions, these are a compact equivalent of expression = value1 OR expression = value2 OR .. OR expression = valueN . When using an expression of this type, two things are required. The types of the test expression and all value expressions that are related must be of the same type. Additionally, a function signature for equality must be available for the expression type used.","title":"Specialized Record Expressions"},{"location":"expressions/specialized_record_expressions/#specialized-record-expressions","text":"While most all types of operations could be reduced to functions, in some cases this would be overly simplistic. Instead, it is helpful to construct some other expression constructs. These constructs should be focused on different expression types as opposed to something that directly related to syntantic sugar. For example, CAST and EXTRACT or SQL operations that are presented using specialized syntax. However, they can easily modeled using a function paradigm with minimal complexity.","title":"Specialized Record Expressions"},{"location":"expressions/specialized_record_expressions/#literal-expressions","text":"For each data type, it is possible to create a literal value for that data type. The representation depends on the serialization format.","title":"Literal Expressions"},{"location":"expressions/specialized_record_expressions/#if-expression","text":"An if value expression is an expression composed of one if clause, zero or more else if clauses and an else clause. In pseudo code, they are envisioned as: if <boolean expression> then <result expression 1> else if <boolean expression> then <result expression 2> (zero or more times) else <result expression 3> When an if expression is declared, all return expressions must be the same identical type.","title":"If Expression"},{"location":"expressions/specialized_record_expressions/#shortcut-behavior","text":"An if expression is expected to logically short-cicuit on a postive outcome. This means that a skipped else/elseif expression cannot cause an error. For example, this should not actually throw an error despite the fact that the cast operation should fail. if 'value' = 'value' then 0 else cast('hello' as integer)","title":"Shortcut Behavior"},{"location":"expressions/specialized_record_expressions/#switch-expression","text":"Switch expression allow a selection of alternate branches based on the value of a given expression. They are an optimized form of a generic if expression where all conditions are equality to the same value. In pseudo-code: switch(value) <value 1> => <return 1> (1 or more times) <else> => <return 3> Return values for a switch expression must all be of identical type.","title":"Switch Expression"},{"location":"expressions/specialized_record_expressions/#shortcut-behavior_1","text":"As in if expressions, switch expression evaluation should not be interrupted by \u201croads not taken\u201d.","title":"Shortcut Behavior"},{"location":"expressions/specialized_record_expressions/#or-list-equality-expression","text":"A specialized structure that is often used is a large list of possible values. In SQL, these are typically large IN lists. They can composed of one or more fields. There are two common patterns, single value and multi value. In pseudo code they are represented as: Single Value: expression, [<value1>, <value2>, ... <valueN>] Multi Value: [expressionA, expressionB], [[value1a, value1b], [value2a, value2b].. [valueNa, valueNb]] For single value expressions, these are a compact equivalent of expression = value1 OR expression = value2 OR .. OR expression = valueN . When using an expression of this type, two things are required. The types of the test expression and all value expressions that are related must be of the same type. Additionally, a function signature for equality must be available for the expression type used.","title":"Or List Equality Expression"},{"location":"expressions/table_functions/","text":"Table Functions \u00b6 Table functions produce zero or more records for each input record. Table functions use a signature similar to scalar functions. However, they are not allowed in the same contexts. to be completed\u2026","title":"Table Functions"},{"location":"expressions/table_functions/#table-functions","text":"Table functions produce zero or more records for each input record. Table functions use a signature similar to scalar functions. However, they are not allowed in the same contexts. to be completed\u2026","title":"Table Functions"},{"location":"expressions/user_defined_functions/","text":"User Defined Functions \u00b6 Substrait supports the creation of custom functions using the function signature facility described in scalar functions . If a user wants to declare their own custom functions, they can do either of the following: Expose their function signatures publically using a public organization id. Public organization ids are between 0 and 2B (exclusive) and are registered with the Substrait repository here . Define one or more private organization ids. Private organization ids are 2B and above. Public organizations should be automatically mapped by tools, private organizations will have to be manually mapped. Once a organization id is defined and mapped, a plan will validate against the function signatures listed in the extensions table of contents file. Example file . Once a function signature is defined, user defined functions are treated exactly the same as normal functions within the plan.","title":"User Defined Functions"},{"location":"expressions/user_defined_functions/#user-defined-functions","text":"Substrait supports the creation of custom functions using the function signature facility described in scalar functions . If a user wants to declare their own custom functions, they can do either of the following: Expose their function signatures publically using a public organization id. Public organization ids are between 0 and 2B (exclusive) and are registered with the Substrait repository here . Define one or more private organization ids. Private organization ids are 2B and above. Public organizations should be automatically mapped by tools, private organizations will have to be manually mapped. Once a organization id is defined and mapped, a plan will validate against the function signatures listed in the extensions table of contents file. Example file . Once a function signature is defined, user defined functions are treated exactly the same as normal functions within the plan.","title":"User Defined Functions"},{"location":"expressions/window_functions/","text":"Window Functions \u00b6 Window functions are functions that define an operation which consumes values from multiple records to a produce a single output. They are similar to aggregate functions but also have a focused window of analysis to compare to their partition window. The function similar to scalar values to an end user, producing a single value for each input record. However, their consumption visibility for each record production can be many records. Window function signatures contain all of the properties defined for aggregate functions . Additionally, they contain the properties below Property Description Required Inherits All properties defined for aggregate functions N/A Window Type STREAMING or PARTITION. Describes whether the function needs to see all data for the specific partition operation simultaneously. Operations like SUM can produce values in a streaming manner with no complete visibility of the partition. NTILE requires visibility of the entire partition before it can start producing values. Optional, defaults to PARTITION When Binding an Aggregate function, the binding must include the following additional properties beyond the standard scalar binding properties: Property Description Required Partition A list of partitioning expressions. False, defaults to a single partition for the entire dataset Lower Bound Bound Following(int64), Bound Trailing(int64) or CurrentRow False, defaults to start of partition Upper Bound Bound Following(int64), Bound Trailing(int64) or CurrentRow False, defaults to end of partition","title":"Window Functions"},{"location":"expressions/window_functions/#window-functions","text":"Window functions are functions that define an operation which consumes values from multiple records to a produce a single output. They are similar to aggregate functions but also have a focused window of analysis to compare to their partition window. The function similar to scalar values to an end user, producing a single value for each input record. However, their consumption visibility for each record production can be many records. Window function signatures contain all of the properties defined for aggregate functions . Additionally, they contain the properties below Property Description Required Inherits All properties defined for aggregate functions N/A Window Type STREAMING or PARTITION. Describes whether the function needs to see all data for the specific partition operation simultaneously. Operations like SUM can produce values in a streaming manner with no complete visibility of the partition. NTILE requires visibility of the entire partition before it can start producing values. Optional, defaults to PARTITION When Binding an Aggregate function, the binding must include the following additional properties beyond the standard scalar binding properties: Property Description Required Partition A list of partitioning expressions. False, defaults to a single partition for the entire dataset Lower Bound Bound Following(int64), Bound Trailing(int64) or CurrentRow False, defaults to start of partition Upper Bound Bound Following(int64), Bound Trailing(int64) or CurrentRow False, defaults to end of partition","title":"Window Functions"},{"location":"relations/basics/","text":"Basics \u00b6 Substrait is designed to allow a user to construct an arbitrarily complex data transformation plan. The plan is composed of one or more relational operations. Relational operations are well-defined transformation operations that work by taking zero or more input datasets and transforming them into zero or more output transformations. Substrait defines a core set of transformations and users are also able to extend the operations with their own specialized operations. Each relational operation is composed of several different properties. Common properties for relational operations include the following: Property Description Type Emit The set of columns output from this operation and the order of those columns. Logical & Physical Hints A set of optionally provided, optionally consumed information about an operation that better inform execution. These might include estimated number of input and output records, estimated record size, likely filter reduction, estimated dictionary size, etc. These can also include implementation specific pieces of execution information. Physical Constraint A set of runtime constraints around the operation, limiting its consumption based on irl resources (CPU, memory) as well as virtual resources like number of records produced, largest record size, etc. Physical Relational Signatures \u00b6 In functions, function signatures are declared externally to the use of those signatures (function bindings). In the case of relational operations, signatures are declared directly in the specification. This is due to the speed of change and number of total operations. Relational operations in the specification are expected to be <100 for several years with additions being infrequent. On the other hand, there is an expectation of both a much larger number of functions (1,000s) and a much higher velocity of additions. Each Relational Operation must declare the following: Transformation logic around properties of the data. For example, does a relational operation maintain sortedness of a field? Does an operation change the distribution of data? How many input sets does an operation produce? Does the operator produce an output (by specification, we limit relational operations to a single output at this time) What is the schema and field ordering of an output (see emit below)? Emit: Output Ordering \u00b6 A relational operation uses field references to access specific fields of the input stream. Field references are always ordinal based on the order of the incoming streams. Each relational operation must declare the order of its output data. To simpify things, each relational operation can be in one of two modes: Direct output : The order of outputs is based on the definition declared by the relational operation. Remap : A listed ordering of the direct outputs. This remapping can be also used to drop columns no longer used (such as a filter field or join keys after a join). Note that remapping/exclusion can only be done at the outputs root struct. Filtering of compound values or extracting subsets must be done through other operation types (e.g. projection). Basic Operations \u00b6 To simplify the discussion, initially we are focused on defining two basic operations for a simple plan. Those operations are reading data from disk and filtering that data. Read Operator \u00b6 The read operator is an operator that produces one output. A simple example would be the reading of a Parquet file. It is expected that many types of reads will be added over time Signature Value Inputs 0 Outputs 1 Property Maintenance N/A (no inputs) Output Order Defaults to the schema of the data read. Properties \u00b6 Property Description Required Read Type The type of read to complete. Required Definition The contents of the read property definition, validated to the read type signature Required Direct Schema Defines the schema of the output of the read (before any emit remapping/hiding). Required Filter A boolean Substrait expression that describes the filter of a iceberg dataset. TBD: define how field referencing works. Optional, defaults to none. Projection A masked complex expression describing the portions of the content that should be read Optional, defaults to all of schema Properties A list of name/value pairs associated with the read Optional, defaults to empty Read Definition Types \u00b6 Read definition types are built by the community and added to the specification. This is a portion of specification that is expected to grow rapidly. Virtual Table \u00b6 Property Description Required Data Required Required Files Type \u00b6 Property Description Required Items An array Items (path or path glob) associated with the read Required Format per item Enumeration of available formats. Only current option is PARQUET. Required Discussion Points \u00b6 Do we try to make read definition types more extensible ala function signatures? Is that necessary if we have a custom relational operator? How do we express decomposed types. For example, the Iceberg type above is for early logical planning. Once we do some operations, it may produce a list of Iceberg file reads. This is likely a secondary type of object. We currently include a generic properties property on read type. Do we want this dumping ground?","title":"Basics"},{"location":"relations/basics/#basics","text":"Substrait is designed to allow a user to construct an arbitrarily complex data transformation plan. The plan is composed of one or more relational operations. Relational operations are well-defined transformation operations that work by taking zero or more input datasets and transforming them into zero or more output transformations. Substrait defines a core set of transformations and users are also able to extend the operations with their own specialized operations. Each relational operation is composed of several different properties. Common properties for relational operations include the following: Property Description Type Emit The set of columns output from this operation and the order of those columns. Logical & Physical Hints A set of optionally provided, optionally consumed information about an operation that better inform execution. These might include estimated number of input and output records, estimated record size, likely filter reduction, estimated dictionary size, etc. These can also include implementation specific pieces of execution information. Physical Constraint A set of runtime constraints around the operation, limiting its consumption based on irl resources (CPU, memory) as well as virtual resources like number of records produced, largest record size, etc. Physical","title":"Basics"},{"location":"relations/basics/#relational-signatures","text":"In functions, function signatures are declared externally to the use of those signatures (function bindings). In the case of relational operations, signatures are declared directly in the specification. This is due to the speed of change and number of total operations. Relational operations in the specification are expected to be <100 for several years with additions being infrequent. On the other hand, there is an expectation of both a much larger number of functions (1,000s) and a much higher velocity of additions. Each Relational Operation must declare the following: Transformation logic around properties of the data. For example, does a relational operation maintain sortedness of a field? Does an operation change the distribution of data? How many input sets does an operation produce? Does the operator produce an output (by specification, we limit relational operations to a single output at this time) What is the schema and field ordering of an output (see emit below)?","title":"Relational Signatures"},{"location":"relations/basics/#emit-output-ordering","text":"A relational operation uses field references to access specific fields of the input stream. Field references are always ordinal based on the order of the incoming streams. Each relational operation must declare the order of its output data. To simpify things, each relational operation can be in one of two modes: Direct output : The order of outputs is based on the definition declared by the relational operation. Remap : A listed ordering of the direct outputs. This remapping can be also used to drop columns no longer used (such as a filter field or join keys after a join). Note that remapping/exclusion can only be done at the outputs root struct. Filtering of compound values or extracting subsets must be done through other operation types (e.g. projection).","title":"Emit: Output Ordering"},{"location":"relations/basics/#basic-operations","text":"To simplify the discussion, initially we are focused on defining two basic operations for a simple plan. Those operations are reading data from disk and filtering that data.","title":"Basic Operations"},{"location":"relations/basics/#read-operator","text":"The read operator is an operator that produces one output. A simple example would be the reading of a Parquet file. It is expected that many types of reads will be added over time Signature Value Inputs 0 Outputs 1 Property Maintenance N/A (no inputs) Output Order Defaults to the schema of the data read.","title":"Read Operator"},{"location":"relations/basics/#properties","text":"Property Description Required Read Type The type of read to complete. Required Definition The contents of the read property definition, validated to the read type signature Required Direct Schema Defines the schema of the output of the read (before any emit remapping/hiding). Required Filter A boolean Substrait expression that describes the filter of a iceberg dataset. TBD: define how field referencing works. Optional, defaults to none. Projection A masked complex expression describing the portions of the content that should be read Optional, defaults to all of schema Properties A list of name/value pairs associated with the read Optional, defaults to empty","title":"Properties"},{"location":"relations/basics/#read-definition-types","text":"Read definition types are built by the community and added to the specification. This is a portion of specification that is expected to grow rapidly.","title":"Read Definition Types"},{"location":"relations/basics/#virtual-table","text":"Property Description Required Data Required Required","title":"Virtual Table"},{"location":"relations/basics/#files-type","text":"Property Description Required Items An array Items (path or path glob) associated with the read Required Format per item Enumeration of available formats. Only current option is PARQUET. Required","title":"Files Type"},{"location":"relations/basics/#discussion-points","text":"Do we try to make read definition types more extensible ala function signatures? Is that necessary if we have a custom relational operator? How do we express decomposed types. For example, the Iceberg type above is for early logical planning. Once we do some operations, it may produce a list of Iceberg file reads. This is likely a secondary type of object. We currently include a generic properties property on read type. Do we want this dumping ground?","title":"Discussion Points"},{"location":"relations/embedded_relations/","text":"Embedded Relations \u00b6 pending","title":"Embedded Relations"},{"location":"relations/embedded_relations/#embedded-relations","text":"pending","title":"Embedded Relations"},{"location":"relations/exchange_relations/","text":"Exchange Relations \u00b6 pending","title":"Exchange Relations"},{"location":"relations/exchange_relations/#exchange-relations","text":"pending","title":"Exchange Relations"},{"location":"relations/logical_relations/","text":"Logical Relations \u00b6","title":"Logical Relations"},{"location":"relations/logical_relations/#logical-relations","text":"","title":"Logical Relations"},{"location":"relations/physical_relations/","text":"Physical Relations \u00b6 pending","title":"Physical Relations"},{"location":"relations/physical_relations/#physical-relations","text":"pending","title":"Physical Relations"},{"location":"relations/user_defined_relations/","text":"User Defined Relations \u00b6 pending","title":"User Defined Relations"},{"location":"relations/user_defined_relations/#user-defined-relations","text":"pending","title":"User Defined Relations"},{"location":"serialization/binary_serialization/","text":"Binary Serialization \u00b6 The binary format of a Substrait is designed to be easy to work with in many languages. A key requirement is that someone can take the binary format IDL and use standard tools to build a set of primitives that are easy to work with in any of a number of languages. This allows communities to build and use Substrait using only a binary IDL and the specification (and allows the Substrait project to avoid being required to build libraries for each language to work with the specification). There are several binary IDLs that exist today. The key requirements for Substrait are the following: Strongly typed IDL schema language High Quality well supported and idiomatic bindings/compilers for key languages (Python, Javascript, C++, Go, Rust, Java) Compact serial representation The primary formats that exist that roughly qualify under these requirements include: Protobuf, Thrift, Flatbuf, Avro, Cap\u2019N\u2019Proto. The current plan is to use Protobuf, primarily due to its clean typing system and large number of high quality language bindings. Flatbuf is a close second but it\u2019s poor support for unions along with the complexity of api use in many languages make it unsuitable to a project that is trying to avoid having to generate per language bindings to grow initial adoption of the core plan specification.","title":"Binary Serialization"},{"location":"serialization/binary_serialization/#binary-serialization","text":"The binary format of a Substrait is designed to be easy to work with in many languages. A key requirement is that someone can take the binary format IDL and use standard tools to build a set of primitives that are easy to work with in any of a number of languages. This allows communities to build and use Substrait using only a binary IDL and the specification (and allows the Substrait project to avoid being required to build libraries for each language to work with the specification). There are several binary IDLs that exist today. The key requirements for Substrait are the following: Strongly typed IDL schema language High Quality well supported and idiomatic bindings/compilers for key languages (Python, Javascript, C++, Go, Rust, Java) Compact serial representation The primary formats that exist that roughly qualify under these requirements include: Protobuf, Thrift, Flatbuf, Avro, Cap\u2019N\u2019Proto. The current plan is to use Protobuf, primarily due to its clean typing system and large number of high quality language bindings. Flatbuf is a close second but it\u2019s poor support for unions along with the complexity of api use in many languages make it unsuitable to a project that is trying to avoid having to generate per language bindings to grow initial adoption of the core plan specification.","title":"Binary Serialization"},{"location":"serialization/text_serialization/","text":"Text Serialization \u00b6 To maximize the new user experience, it is important for Substrait to have a text representation of plans. This allows people to experiment with basic tooling. Building simple CLI tools that do things like SQL > Plan and Plan > SQL or REPL plan construction can all be done relatively straightforwardly with a text representation. The recommended text serialization format is JSON. Since the text format is not designed for performance, the format can be produced to maximize readability. This also allows nice symmetry between the construction of plans and the configuration of various extensions such as function signatures and user defined types. To ensure the JSON is valid, the object will defined using the OpenApi 3.1 specification . This not only allows strong validation, the openapi specification enables code generators to be easily used to produce plans in many languages. While JSON will be used for much of the plan serialization, Substrait uses a custom simplistic grammar for record level expressions. While one can construct an equation such as (10 + 5)/2 using a tree of function and literal objects, it is much more human readable to consume a plan when the information is written similarly to the way one typically consumes scalar expressions. This grammar will be maintained in an ANTLR grammar (targetable to multiple programming languages) and is also planned to be supported via JSON schema definition format tag so that the grammar can be validated as part of the schema validation.","title":"Text Serialization"},{"location":"serialization/text_serialization/#text-serialization","text":"To maximize the new user experience, it is important for Substrait to have a text representation of plans. This allows people to experiment with basic tooling. Building simple CLI tools that do things like SQL > Plan and Plan > SQL or REPL plan construction can all be done relatively straightforwardly with a text representation. The recommended text serialization format is JSON. Since the text format is not designed for performance, the format can be produced to maximize readability. This also allows nice symmetry between the construction of plans and the configuration of various extensions such as function signatures and user defined types. To ensure the JSON is valid, the object will defined using the OpenApi 3.1 specification . This not only allows strong validation, the openapi specification enables code generators to be easily used to produce plans in many languages. While JSON will be used for much of the plan serialization, Substrait uses a custom simplistic grammar for record level expressions. While one can construct an equation such as (10 + 5)/2 using a tree of function and literal objects, it is much more human readable to consume a plan when the information is written similarly to the way one typically consumes scalar expressions. This grammar will be maintained in an ANTLR grammar (targetable to multiple programming languages) and is also planned to be supported via JSON schema definition format tag so that the grammar can be validated as part of the schema validation.","title":"Text Serialization"},{"location":"spec/specification/","text":"Specification \u00b6 Process \u00b6 The goal of this project is initially to establish a well-defined specification. Once established, new versions of the specification will follow a normal development/release process. To provide something to peruse while clarifying an openness to the community during the initial development of the specification, we plan to follow the following steps for development of the specification. We will use github branches to describe each of these steps and patches will be proposed to be moved from one branch to the next to allow review of documents while still having strawmen to start with. The steps are: Empty - No outline has been produced. A sketch needs to be produced for people to react and iterate on. Sketch - Something has been written but should serve more as a conceptual backing for what should be achieved in this part of the specification. No collaboration or consensus has occurred. This will be discussed and iterated on until an initial WIP version can be patched. The WIP version will be held in a PR to iterate on until it is committed to the WIP branch of the repository. WIP - An initial version that multiple contributors have agreed to has been produced for this portion of the specification. Any user is welcome to propose additional changes or discussions with regards to this component but it now represents a community intention. Commit - Believed to be a well-formed plan for this portion of the specification. Documents that have had no outstanding reviews for 14 days will be moved from WIP to commit. Changes can still be made but the section should no longer be under constant revision. (This status is more for external observers to understand the progress of the specification than something that influences internal project process.) Once all portions of the specification have been moved to commit (or eliminated), the specification will move to an initial version number. To try to get a working end-to-end model as quickly as possible, a small number of items have been prioritized. The set of components outlined here are proposed as a mechanism for having bite-size review/discussion chunks to make forward progress. Components \u00b6 Priority Status Section Description 1 sketch Simple Logical Types A way to describe the set of basic types that will be operated on within a plan. Only includes simple types such as integers and doubles (nothing configurable or compound). sketch Compound Logical Types Expression of types that go beyond simple scalar values. Key concepts here include: configurable types such as fixed length and numeric types as well as compound types such as structs, maps, lists, etc. sketch Physical Types Physical extensions to logical types. sketch User Defined Types Extensions that can be defined for specific IR producers/consumers. 2 sketch Field References Expressions to identify which portions of a record should be 3 sketch Scalar Functions Description of how functions are specified. Concepts include arguments, variadic functions, output type derivation, etc. sketch Scalar Function List A list of well-known canonical functions in yaml format. sketch Specialized Record Expressions Specialized expression types that are more naturally expressed outside the function paradigm. Examples include items such as if/then/else and switch statements. sketch Aggregate Functions Functions that are expressed in aggregation operations. Examples include things such as SUM, COUNT, etc. Operations take many records and collapse them into a single (possibly compound) value. sketch Window Functions Functions that relate a record to a set of encompassing records. Examples in SQL include RANK, NTILE, etc. empty Table Functions Functions that convert one or more values from an input record into 0..N output records. Example include operations such as explode, pos-exlode, etc. sketch User Defined Functions Reusable named functions that are built beyond the core specification. Implementations are typically registered thorugh external means (drop a file in a directory, send a special command with implementation, etc). sketch Embedded Functions Functions implementations embedded directly within the plan. Frequently used in data scicence workflows where business logic is interpersed with standard operations. 4 sketch Relation Basics Basic concepts around relational algebra and the two basic relational operations: read & filter. empty Logical Relations Common relational operations used in compute plans including project, join, aggregation, etc. empty Physical Relations Specific execution sub-variations of common relational operations that describe how an operation should be down. Examples include hash join, merge join, nested loop join, etc. empty Exchange Relations Operations associated with distributing work between multiple systems. Example might include hashed send, unordered reception, etc. empty User Defined Relations Installed and reusable relational operations customized to a particular platform. empty Embedded Relations Relational operations where plans contain the \u201cmachine code\u201d to directly execute the necessary operations. 5 sketch Text Serialization A human producable & consumable representation of the plan specification. 6 sketch Binary Serialization A high performance & compact binary representation of the plan specification.","title":"Specification"},{"location":"spec/specification/#specification","text":"","title":"Specification"},{"location":"spec/specification/#process","text":"The goal of this project is initially to establish a well-defined specification. Once established, new versions of the specification will follow a normal development/release process. To provide something to peruse while clarifying an openness to the community during the initial development of the specification, we plan to follow the following steps for development of the specification. We will use github branches to describe each of these steps and patches will be proposed to be moved from one branch to the next to allow review of documents while still having strawmen to start with. The steps are: Empty - No outline has been produced. A sketch needs to be produced for people to react and iterate on. Sketch - Something has been written but should serve more as a conceptual backing for what should be achieved in this part of the specification. No collaboration or consensus has occurred. This will be discussed and iterated on until an initial WIP version can be patched. The WIP version will be held in a PR to iterate on until it is committed to the WIP branch of the repository. WIP - An initial version that multiple contributors have agreed to has been produced for this portion of the specification. Any user is welcome to propose additional changes or discussions with regards to this component but it now represents a community intention. Commit - Believed to be a well-formed plan for this portion of the specification. Documents that have had no outstanding reviews for 14 days will be moved from WIP to commit. Changes can still be made but the section should no longer be under constant revision. (This status is more for external observers to understand the progress of the specification than something that influences internal project process.) Once all portions of the specification have been moved to commit (or eliminated), the specification will move to an initial version number. To try to get a working end-to-end model as quickly as possible, a small number of items have been prioritized. The set of components outlined here are proposed as a mechanism for having bite-size review/discussion chunks to make forward progress.","title":"Process"},{"location":"spec/specification/#components","text":"Priority Status Section Description 1 sketch Simple Logical Types A way to describe the set of basic types that will be operated on within a plan. Only includes simple types such as integers and doubles (nothing configurable or compound). sketch Compound Logical Types Expression of types that go beyond simple scalar values. Key concepts here include: configurable types such as fixed length and numeric types as well as compound types such as structs, maps, lists, etc. sketch Physical Types Physical extensions to logical types. sketch User Defined Types Extensions that can be defined for specific IR producers/consumers. 2 sketch Field References Expressions to identify which portions of a record should be 3 sketch Scalar Functions Description of how functions are specified. Concepts include arguments, variadic functions, output type derivation, etc. sketch Scalar Function List A list of well-known canonical functions in yaml format. sketch Specialized Record Expressions Specialized expression types that are more naturally expressed outside the function paradigm. Examples include items such as if/then/else and switch statements. sketch Aggregate Functions Functions that are expressed in aggregation operations. Examples include things such as SUM, COUNT, etc. Operations take many records and collapse them into a single (possibly compound) value. sketch Window Functions Functions that relate a record to a set of encompassing records. Examples in SQL include RANK, NTILE, etc. empty Table Functions Functions that convert one or more values from an input record into 0..N output records. Example include operations such as explode, pos-exlode, etc. sketch User Defined Functions Reusable named functions that are built beyond the core specification. Implementations are typically registered thorugh external means (drop a file in a directory, send a special command with implementation, etc). sketch Embedded Functions Functions implementations embedded directly within the plan. Frequently used in data scicence workflows where business logic is interpersed with standard operations. 4 sketch Relation Basics Basic concepts around relational algebra and the two basic relational operations: read & filter. empty Logical Relations Common relational operations used in compute plans including project, join, aggregation, etc. empty Physical Relations Specific execution sub-variations of common relational operations that describe how an operation should be down. Examples include hash join, merge join, nested loop join, etc. empty Exchange Relations Operations associated with distributing work between multiple systems. Example might include hashed send, unordered reception, etc. empty User Defined Relations Installed and reusable relational operations customized to a particular platform. empty Embedded Relations Relational operations where plans contain the \u201cmachine code\u201d to directly execute the necessary operations. 5 sketch Text Serialization A human producable & consumable representation of the plan specification. 6 sketch Binary Serialization A high performance & compact binary representation of the plan specification.","title":"Components"},{"location":"spec/technology_principles/","text":"Substrait \u00b6 Project Vision \u00b6 Define a well-defined, cross-language specification of data compute operations. This includes a declaration of common operations, custom operations and one or more serialized representations of this specification. The spec focuses on the semantics of each operation and a consistent way to describe. In many ways, the goal of this project is similar to that of the Apache Arrow project. Arrow is focused on a standardized memory representation of columnar data. Substrait is focused on what should be done to data. Example Use Cases \u00b6 Communicate a compute plan between a SQL parser and an execution engine (e.g. Calcite SQL parsing to Arrow C++ compute kernel) Serialize a plan that represents a view for consumption in multiple systems (e.g. Iceberg views in Spark and Trino) Create an alternative plan generation implementation that can connect an existing end-user compute expression system to an existing end-user processing engine (e.g. Pandas operations executed inside SingleStore) Build a pluggable plan visualization tool (e.g. D3 based plan visualizer) Community Principles \u00b6 Be inclusive and open to all. If you want to join the project, open a PR or issue or join the Slack channel (TBD) Ensure a diverse set of contributors that come from multiple data backgrounds to maximize general utility. Build a specification based on open consensus. Make the specification and all tools freely available on a permissive license (ApacheV2) Technology Principles \u00b6 Provide a good suite of well-specified common functionality in databases and data science applications. Make it easy for users to privately or publically extend the representation to support specialized/custom operations. Produce something that is language agnostic and requires minimal work to start developing against in a new language. Drive towards a common format that avoids specialization for single favorite producer or consumer. Establish clear delineation between specifications that MUST be respected to and those that can be optionally ignored. Establish a forgiving compatibility approach and versioning scheme that supports cross-version compatibility in maximum number of cases. Minimize the need for consumer intelligence by excluding concepts like overloading, type coercion, implicit casting, field name handling, etc. (Note: this is weak and should be better stated.) Decomposability/severability: A particular producer or consumer should be able to produce or consume only a subset of the specification and interact well with any other Substrait system as long the specific operations requested fit within the subset of specification supported by the counter system. Development Process \u00b6 The goal of this project is initially to establish a well-defined specification. Once established, new versions of the specification will follow a normal development/release process. To provide something to peruse while clarifying an openness to the community during the initial development of the specification, we plan to follow the following steps for development of the specification. We will use github branches to describe each of these steps and patches will be proposed to be moved from one branch to the next to allow review of documents while still having strawmen to start with. The steps are: Empty - No outline has been produced. A sketch needs to be produced for people to react and iterate on. Sketch - Something has been written but should serve more as a conceptual backing for what should be achieved in this part of the specification. No collaboration or consensus has occurred. This will be discussed and iterated on until an initial WIP version can be patched. The WIP version will be held in a PR to iterate on until it is committed to the WIP branch of the repository. WIP - An initial version that multiple contributors have agreed to has been produced for this portion of the specification. Any user is welcome to propose additional changes or discussions with regards to this component but it now represents a community intention. Commit - Believed to be a well-formed plan for this portion of the specification. Documents that have had no outstanding reviews for 14 days will be moved from WIP to commit. Changes can still be made but the section should no longer be under constant revision. (This status is more for external observers to understand the progress of the specification than something that influences internal project process.) Once all portions of the specification have been moved to commit (or eliminated), the specification will move to an initial version number. To try to get a working end-to-end model as quickly as possible, a small number of items have been prioritized. The set of components outlined here are proposed as a mechanism for having bite-size review/discussion chunks to make forward progress. Specification Components \u00b6 Priority Status Section Description 1 sketch Simple Logical Types A way to describe the set of basic types that will be operated on within a plan. Only includes simple types such as integers and doubles (nothing configurable or compound). sketch Compound Logical Types Expression of types that go beyond simple scalar values. Key concepts here include: configurable types such as fixed length and numeric types as well as compound types such as structs, maps, lists, etc. sketch Physical Types Physical extensions to logical types. sketch User Defined Types Extensions that can be defined for specific IR producers/consumers. 2 sketch Field References Expressions to identify which portions of a record should be 3 sketch Scalar Functions Description of how functions are specified. Concepts include arguments, variadic functions, output type derivation, etc. sketch Scalar Function List A list of well-known canonical functions in yaml format. sketch Specialized Record Expressions Specialized expression types that are more naturally expressed outside the function paradigm. Examples include items such as if/then/else and switch statements. sketch Aggregate Functions Functions that are expressed in aggregation operations. Examples include things such as SUM, COUNT, etc. Operations take many records and collapse them into a single (possibly compound) value. sketch Window Functions Functions that relate a record to a set of encompassing records. Examples in SQL include RANK, NTILE, etc. empty Table Functions Functions that convert one or more values from an input record into 0..N output records. Example include operations such as explode, pos-exlode, etc. sketch User Defined Functions Reusable named functions that are built beyond the core specification. Implementations are typically registered thorugh external means (drop a file in a directory, send a special command with implementation, etc). sketch Embedded Functions Functions implementations embedded directly within the plan. Frequently used in data scicence workflows where business logic is interpersed with standard operations. 4 sketch Relational Basics Basic concepts around relational algebra and the two basic relational operations: read & filter. empty Logical Operations Common relational operations used in compute plans including project, join, aggregation, etc. empty Physical Relational Operations Specific execution sub-variations of common relational operations that describe how an operation should be down. Examples include hash join, merge join, nested loop join, etc. empty Exchange Operations Operations associated with distributing work between multiple systems. Example might include hashed send, unordered reception, etc. empty User Defined Relational Operations Installed and reusable relational operations customized to a particular platform. empty Embedded Relational Operations Relational operations where plans contain the \u201cmachine code\u201d to directly execute the necessary operations. 5 sketch Text Serialization A human producable & consumable representation of the plan specification. 6 sketch Binary Serialization A high performance & compact binary representation of the plan specification.","title":"Substrait"},{"location":"spec/technology_principles/#substrait","text":"","title":"Substrait"},{"location":"spec/technology_principles/#project-vision","text":"Define a well-defined, cross-language specification of data compute operations. This includes a declaration of common operations, custom operations and one or more serialized representations of this specification. The spec focuses on the semantics of each operation and a consistent way to describe. In many ways, the goal of this project is similar to that of the Apache Arrow project. Arrow is focused on a standardized memory representation of columnar data. Substrait is focused on what should be done to data.","title":"Project Vision"},{"location":"spec/technology_principles/#example-use-cases","text":"Communicate a compute plan between a SQL parser and an execution engine (e.g. Calcite SQL parsing to Arrow C++ compute kernel) Serialize a plan that represents a view for consumption in multiple systems (e.g. Iceberg views in Spark and Trino) Create an alternative plan generation implementation that can connect an existing end-user compute expression system to an existing end-user processing engine (e.g. Pandas operations executed inside SingleStore) Build a pluggable plan visualization tool (e.g. D3 based plan visualizer)","title":"Example Use Cases"},{"location":"spec/technology_principles/#community-principles","text":"Be inclusive and open to all. If you want to join the project, open a PR or issue or join the Slack channel (TBD) Ensure a diverse set of contributors that come from multiple data backgrounds to maximize general utility. Build a specification based on open consensus. Make the specification and all tools freely available on a permissive license (ApacheV2)","title":"Community Principles"},{"location":"spec/technology_principles/#technology-principles","text":"Provide a good suite of well-specified common functionality in databases and data science applications. Make it easy for users to privately or publically extend the representation to support specialized/custom operations. Produce something that is language agnostic and requires minimal work to start developing against in a new language. Drive towards a common format that avoids specialization for single favorite producer or consumer. Establish clear delineation between specifications that MUST be respected to and those that can be optionally ignored. Establish a forgiving compatibility approach and versioning scheme that supports cross-version compatibility in maximum number of cases. Minimize the need for consumer intelligence by excluding concepts like overloading, type coercion, implicit casting, field name handling, etc. (Note: this is weak and should be better stated.) Decomposability/severability: A particular producer or consumer should be able to produce or consume only a subset of the specification and interact well with any other Substrait system as long the specific operations requested fit within the subset of specification supported by the counter system.","title":"Technology Principles"},{"location":"spec/technology_principles/#development-process","text":"The goal of this project is initially to establish a well-defined specification. Once established, new versions of the specification will follow a normal development/release process. To provide something to peruse while clarifying an openness to the community during the initial development of the specification, we plan to follow the following steps for development of the specification. We will use github branches to describe each of these steps and patches will be proposed to be moved from one branch to the next to allow review of documents while still having strawmen to start with. The steps are: Empty - No outline has been produced. A sketch needs to be produced for people to react and iterate on. Sketch - Something has been written but should serve more as a conceptual backing for what should be achieved in this part of the specification. No collaboration or consensus has occurred. This will be discussed and iterated on until an initial WIP version can be patched. The WIP version will be held in a PR to iterate on until it is committed to the WIP branch of the repository. WIP - An initial version that multiple contributors have agreed to has been produced for this portion of the specification. Any user is welcome to propose additional changes or discussions with regards to this component but it now represents a community intention. Commit - Believed to be a well-formed plan for this portion of the specification. Documents that have had no outstanding reviews for 14 days will be moved from WIP to commit. Changes can still be made but the section should no longer be under constant revision. (This status is more for external observers to understand the progress of the specification than something that influences internal project process.) Once all portions of the specification have been moved to commit (or eliminated), the specification will move to an initial version number. To try to get a working end-to-end model as quickly as possible, a small number of items have been prioritized. The set of components outlined here are proposed as a mechanism for having bite-size review/discussion chunks to make forward progress.","title":"Development Process"},{"location":"spec/technology_principles/#specification-components","text":"Priority Status Section Description 1 sketch Simple Logical Types A way to describe the set of basic types that will be operated on within a plan. Only includes simple types such as integers and doubles (nothing configurable or compound). sketch Compound Logical Types Expression of types that go beyond simple scalar values. Key concepts here include: configurable types such as fixed length and numeric types as well as compound types such as structs, maps, lists, etc. sketch Physical Types Physical extensions to logical types. sketch User Defined Types Extensions that can be defined for specific IR producers/consumers. 2 sketch Field References Expressions to identify which portions of a record should be 3 sketch Scalar Functions Description of how functions are specified. Concepts include arguments, variadic functions, output type derivation, etc. sketch Scalar Function List A list of well-known canonical functions in yaml format. sketch Specialized Record Expressions Specialized expression types that are more naturally expressed outside the function paradigm. Examples include items such as if/then/else and switch statements. sketch Aggregate Functions Functions that are expressed in aggregation operations. Examples include things such as SUM, COUNT, etc. Operations take many records and collapse them into a single (possibly compound) value. sketch Window Functions Functions that relate a record to a set of encompassing records. Examples in SQL include RANK, NTILE, etc. empty Table Functions Functions that convert one or more values from an input record into 0..N output records. Example include operations such as explode, pos-exlode, etc. sketch User Defined Functions Reusable named functions that are built beyond the core specification. Implementations are typically registered thorugh external means (drop a file in a directory, send a special command with implementation, etc). sketch Embedded Functions Functions implementations embedded directly within the plan. Frequently used in data scicence workflows where business logic is interpersed with standard operations. 4 sketch Relational Basics Basic concepts around relational algebra and the two basic relational operations: read & filter. empty Logical Operations Common relational operations used in compute plans including project, join, aggregation, etc. empty Physical Relational Operations Specific execution sub-variations of common relational operations that describe how an operation should be down. Examples include hash join, merge join, nested loop join, etc. empty Exchange Operations Operations associated with distributing work between multiple systems. Example might include hashed send, unordered reception, etc. empty User Defined Relational Operations Installed and reusable relational operations customized to a particular platform. empty Embedded Relational Operations Relational operations where plans contain the \u201cmachine code\u201d to directly execute the necessary operations. 5 sketch Text Serialization A human producable & consumable representation of the plan specification. 6 sketch Binary Serialization A high performance & compact binary representation of the plan specification.","title":"Specification Components"},{"location":"types/compound_logical_types/","text":"Logical Compound Types \u00b6 Compound types include any type that is configurable including complex types as well as configurable scalar types. Type Name Description Arrow Analog Iceberg Analog Spark Analog Trino Analog FIXEDCHAR(L) A fixed length field of length L. L can be between [1..2,147,483,647]. Values less that are less in length than the length of the field are padded with spaces. None None CharType(L) CHAR(L) VARCHAR(L) A field that can holds UTF8 encoded strings between 0 and L length. The length of each value can be between [0..2,147,483,647]. The value of L can be between [1..2,147,483,647]. Values shorter than L are not padded. None None VarcharType(L) VARCHAR(L) FIXEDBINARY(L) A binary field that is fixed in width to L. Values that are shorter than L are 0-byte padded. FixedSizeBinary FIXED - - DECIMAL(P,S) Fixed point decimal with precision P and scale S. Precision must be 38 or less. DECIMAL(P,S) DECIMAL(P,S) DECIMAL(P,S) DECIMAL(P,S) STRUCT<N:T1,\u2026,N:T2> A struct that maps unique names to value types. Each name is a UTF8 string. Each value can have a distinct type. struct_<*> struct<*> struct<*> row<*> LIST<T> A list of values of type T. The list can be between [0..2,147,483,647] values in length. Maps to the list list list array MAP An unordered list of type K keys with type V values. map map - map TIMESTAMP_MICRO_TZ(TZ) A timestamp in microseconds with a timezone TZ. timestamp timestamptz - timestamp(6) with time zone TIMESTAMP_MILLI_TZ(TZ) A timestamp in microseconds with a timezone TZ. timestamp - - timestamp(3) with time zone Discussion Points \u00b6 Should union type be included (only exists in Arrow)? Isn\u2019t a named struct sufficient? Arbitrary precision time/date types (e.g. timestamp(9) which exists in Arrow and Trino, timestamp(1) which exists in Trino) Can maps contain multiple values with the same key?","title":"Logical Compound Types"},{"location":"types/compound_logical_types/#logical-compound-types","text":"Compound types include any type that is configurable including complex types as well as configurable scalar types. Type Name Description Arrow Analog Iceberg Analog Spark Analog Trino Analog FIXEDCHAR(L) A fixed length field of length L. L can be between [1..2,147,483,647]. Values less that are less in length than the length of the field are padded with spaces. None None CharType(L) CHAR(L) VARCHAR(L) A field that can holds UTF8 encoded strings between 0 and L length. The length of each value can be between [0..2,147,483,647]. The value of L can be between [1..2,147,483,647]. Values shorter than L are not padded. None None VarcharType(L) VARCHAR(L) FIXEDBINARY(L) A binary field that is fixed in width to L. Values that are shorter than L are 0-byte padded. FixedSizeBinary FIXED - - DECIMAL(P,S) Fixed point decimal with precision P and scale S. Precision must be 38 or less. DECIMAL(P,S) DECIMAL(P,S) DECIMAL(P,S) DECIMAL(P,S) STRUCT<N:T1,\u2026,N:T2> A struct that maps unique names to value types. Each name is a UTF8 string. Each value can have a distinct type. struct_<*> struct<*> struct<*> row<*> LIST<T> A list of values of type T. The list can be between [0..2,147,483,647] values in length. Maps to the list list list array MAP An unordered list of type K keys with type V values. map map - map TIMESTAMP_MICRO_TZ(TZ) A timestamp in microseconds with a timezone TZ. timestamp timestamptz - timestamp(6) with time zone TIMESTAMP_MILLI_TZ(TZ) A timestamp in microseconds with a timezone TZ. timestamp - - timestamp(3) with time zone","title":"Logical Compound Types"},{"location":"types/compound_logical_types/#discussion-points","text":"Should union type be included (only exists in Arrow)? Isn\u2019t a named struct sufficient? Arbitrary precision time/date types (e.g. timestamp(9) which exists in Arrow and Trino, timestamp(1) which exists in Trino) Can maps contain multiple values with the same key?","title":"Discussion Points"},{"location":"types/physical_types/","text":"Physical Types \u00b6 Since Substrait is designed to work in both logical and physical contexts, there is need to support extended attributes in the physical context. For each logical type, we declare one or more physical representations of that logical type as approrpriate to the system specializations. Additionally, we describe whether a particular type is dictionary encoded. Each of these representation details is also used when specifiying a function signature to determine which of the specific physical representations of data are supported by a paticular function signature. In many cases, a system will only have a single representation physical representation of a type. In those cases, it is expected that the binding of an operation is associated with the system default representation of the data. Logical Type Physical Representations Support Dictionary Encoding boolean 0=System default no i8 0=System default no u8 0=System default no i16 0=System default no u16 0=System default no i32 0=System default no u32 0=System default no i64 0=System default no u64 0=System default no fp16 0=System default no fp32 0=System default no fp64 0=System default no string 0=System default, 1=Arrow Large String yes binary 0=System default, Arrow Large Binary yes timestamp_micro 0=System default no timestamp_milli 0=System default no date 0=System default no date_micro 0=System default no time_micro 0=System default no time_milli 0=System default no interval_year 0=System default no interval_day 0=System default, 1=Arrow MONTH_DAY_NANO no fixedchar 0=System default yes varchar 0=System default yes fixedbinary 0=System default yes decimal 0=System default, 1=Arrow 128 Bit Width no struct 0=System default yes list 0=System default, 1=Arrow Large List yes map 0=System default, 1=Map where keys are utf8 ordered strings yes timestamp_micro_tz 0=System default no timestamp_milli_tz 0=System default no","title":"Physical Types"},{"location":"types/physical_types/#physical-types","text":"Since Substrait is designed to work in both logical and physical contexts, there is need to support extended attributes in the physical context. For each logical type, we declare one or more physical representations of that logical type as approrpriate to the system specializations. Additionally, we describe whether a particular type is dictionary encoded. Each of these representation details is also used when specifiying a function signature to determine which of the specific physical representations of data are supported by a paticular function signature. In many cases, a system will only have a single representation physical representation of a type. In those cases, it is expected that the binding of an operation is associated with the system default representation of the data. Logical Type Physical Representations Support Dictionary Encoding boolean 0=System default no i8 0=System default no u8 0=System default no i16 0=System default no u16 0=System default no i32 0=System default no u32 0=System default no i64 0=System default no u64 0=System default no fp16 0=System default no fp32 0=System default no fp64 0=System default no string 0=System default, 1=Arrow Large String yes binary 0=System default, Arrow Large Binary yes timestamp_micro 0=System default no timestamp_milli 0=System default no date 0=System default no date_micro 0=System default no time_micro 0=System default no time_milli 0=System default no interval_year 0=System default no interval_day 0=System default, 1=Arrow MONTH_DAY_NANO no fixedchar 0=System default yes varchar 0=System default yes fixedbinary 0=System default yes decimal 0=System default, 1=Arrow 128 Bit Width no struct 0=System default yes list 0=System default, 1=Arrow Large List yes map 0=System default, 1=Map where keys are utf8 ordered strings yes timestamp_micro_tz 0=System default no timestamp_milli_tz 0=System default no","title":"Physical Types"},{"location":"types/simple_logical_types/","text":"Simple Logical Types \u00b6 Substrait tries to cover the most common types used in data manipulation. Simple types are those that don\u2019t support any form of configuration. For simplicity, any generic type that has only a small number of discrete implementations is declared directly (as opposed to via configuration). To minimize type explosion, the project currently follows the guideline that a logical type should probably only be included in the specification if it is included in at least two of the following open source Projects: Apache Arrow, Apache Iceberg, Apache Spark and Trino. Type Name Description Arrow Analog Iceberg Analog Spark Analog Trino Analog boolean A value that is either true or false. Bool boolean boolean boolean i8 A signed 8 byte value in [-128..127] Int<8,true> - ByteType tinyint u8 An unsigned 8 byte value between [0..255] Int<8,false> - - i16 A signed 16 byte value between [-32,768..32,767] Int<16,true> - ShortType smallint u16 An unsigned 16 byte value between [0..65,535] Int<16,false> - i32 A signed 32 byte value between [-2147483648..2,147,483,647] Int<32,true> int IntegerType int u32 An unsigned 32 byte value between [0..4,294,967,295] Int<32,false> - i64 A signed 64 byte value between [\u22129,223,372,036,854,775,808..9,223,372,036,854,775,807] Int<64,true> long LongType bigint u64 An unsigned 64 byte value between [0..18,446,744,073,709,551,615] Int<64,false> - - - fp16 A 2 byte floating point number with range as defined here . Float - - - fp32 A 4 byte single precision floating point number with range as defined here . Float float FloatType real fp64 An 8 byte double precision floating point number with range as defined here . Float double DecimalType double string A string of text that can be up to 2,147,483,647 bytes in length. String is encoded in UTF8 Utf8 string StringType varchar (no len) binary A binary value that can be up to 2,147,483,647 bytes in length. Binary binary BinaryType Varbinary timestamp_micro A timestamp with microsecond precision Timestamp timestamp TimestampType timestamp(6) timestamp_milli A timestamp with millisecond precision Timestamp - - timestamp(3) date Date, expressed as number of seconds since epoch Date date DateType Date time_micro A time expressed in microseconds since start of day Time time time(6) time(6) time_milli A time expressed in milliseconds since start of day Time - time(3) time(3) interval_year Interval day to month INTERVAL - - Interval year to month interval_day Interval day to second INTERVAL - - Interval day to second Discussion points \u00b6 How do we ensure Substrait is adoptable by different communities while avoiding type explosion? Is it important to avoid type explosion? Current proposal is to avoid types unless they exist in at least two projects. Does it make sense to support user defined types? We\u2019ve included unsigned types here but they only currently exist in Arrow. Should we remove?","title":"Simple Logical Types"},{"location":"types/simple_logical_types/#simple-logical-types","text":"Substrait tries to cover the most common types used in data manipulation. Simple types are those that don\u2019t support any form of configuration. For simplicity, any generic type that has only a small number of discrete implementations is declared directly (as opposed to via configuration). To minimize type explosion, the project currently follows the guideline that a logical type should probably only be included in the specification if it is included in at least two of the following open source Projects: Apache Arrow, Apache Iceberg, Apache Spark and Trino. Type Name Description Arrow Analog Iceberg Analog Spark Analog Trino Analog boolean A value that is either true or false. Bool boolean boolean boolean i8 A signed 8 byte value in [-128..127] Int<8,true> - ByteType tinyint u8 An unsigned 8 byte value between [0..255] Int<8,false> - - i16 A signed 16 byte value between [-32,768..32,767] Int<16,true> - ShortType smallint u16 An unsigned 16 byte value between [0..65,535] Int<16,false> - i32 A signed 32 byte value between [-2147483648..2,147,483,647] Int<32,true> int IntegerType int u32 An unsigned 32 byte value between [0..4,294,967,295] Int<32,false> - i64 A signed 64 byte value between [\u22129,223,372,036,854,775,808..9,223,372,036,854,775,807] Int<64,true> long LongType bigint u64 An unsigned 64 byte value between [0..18,446,744,073,709,551,615] Int<64,false> - - - fp16 A 2 byte floating point number with range as defined here . Float - - - fp32 A 4 byte single precision floating point number with range as defined here . Float float FloatType real fp64 An 8 byte double precision floating point number with range as defined here . Float double DecimalType double string A string of text that can be up to 2,147,483,647 bytes in length. String is encoded in UTF8 Utf8 string StringType varchar (no len) binary A binary value that can be up to 2,147,483,647 bytes in length. Binary binary BinaryType Varbinary timestamp_micro A timestamp with microsecond precision Timestamp timestamp TimestampType timestamp(6) timestamp_milli A timestamp with millisecond precision Timestamp - - timestamp(3) date Date, expressed as number of seconds since epoch Date date DateType Date time_micro A time expressed in microseconds since start of day Time time time(6) time(6) time_milli A time expressed in milliseconds since start of day Time - time(3) time(3) interval_year Interval day to month INTERVAL - - Interval year to month interval_day Interval day to second INTERVAL - - Interval day to second","title":"Simple Logical Types"},{"location":"types/simple_logical_types/#discussion-points","text":"How do we ensure Substrait is adoptable by different communities while avoiding type explosion? Is it important to avoid type explosion? Current proposal is to avoid types unless they exist in at least two projects. Does it make sense to support user defined types? We\u2019ve included unsigned types here but they only currently exist in Arrow. Should we remove?","title":"Discussion points"},{"location":"types/user_defined_types/","text":"User Defined Types \u00b6 User Defined Types can be created using a combination of pre-defined simple and compound types. User defined types are defined as an extension and are classified by the organization that they belong. An organization can declare an arbitrary number of user defined extension types. Initially, user defined types must be simple types (although they can be constructed of a number of inner compound and simple types). A yaml example of an extension type is below: name : point structure : longitude : i32 latitude : i32 This declares a new type (namespaced to the associated organization) called \u201cpoint\u201d. This type is composed of two i32 values named longitude and latitude. Once a type has been declared, it can be used in function declarations. [TBD: should field references be allowed to dereference the components of a user defined type?]","title":"User Defined Types"},{"location":"types/user_defined_types/#user-defined-types","text":"User Defined Types can be created using a combination of pre-defined simple and compound types. User defined types are defined as an extension and are classified by the organization that they belong. An organization can declare an arbitrary number of user defined extension types. Initially, user defined types must be simple types (although they can be constructed of a number of inner compound and simple types). A yaml example of an extension type is below: name : point structure : longitude : i32 latitude : i32 This declares a new type (namespaced to the associated organization) called \u201cpoint\u201d. This type is composed of two i32 values named longitude and latitude. Once a type has been declared, it can be used in function declarations. [TBD: should field references be allowed to dereference the components of a user defined type?]","title":"User Defined Types"}]}