{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Substrait: Cross-Language Serialization for Relational Algebra \u00b6 Project Vision \u00b6 Create a well-defined, cross-language specification for data compute operations. This includes a declaration of common operations, custom operations and one or more serialized representations of this specification. The spec focuses on the semantics of each operation and a consistent way to describe. In many ways, the goal of this project is similar to that of the Apache Arrow project. Arrow is focused on a standardized memory representation of columnar data. Substrait is focused on what should be done to data. Example Use Cases \u00b6 Communicate a compute plan between a SQL parser and an execution engine (e.g. Calcite SQL parsing to Arrow C++ compute kernel) Serialize a plan that represents a SQL view for consistent use in multiple systems (e.g. Iceberg views in Spark and Trino) Submit a plan to different execution engines (e.g. Datafusion and Postgres) and get a consistent interpretation of the semantics. Create an alternative plan generation implementation that can connect an existing end-user compute expression system to an existing end-user processing engine (e.g. Pandas operations executed inside SingleStore) Build a pluggable plan visualization tool (e.g. D3 based plan visualizer) Community Principles \u00b6 Be inclusive and open to all. If you want to join the project, open a PR or issue , start a discussion or join the Slack Channel . Ensure a diverse set of contributors that come from multiple data backgrounds to maximize general utility. Build a specification based on open consensus. Avoid over-reliance/coupling to any single technology. Make the specification and all tools freely available on a permissive license (ApacheV2) Related Technologies \u00b6 Apache Calcite : Many ideas in Substrait are inspired by the Calcite project. Calcite is a great JVM-based sql query parsing and optimization framework. A key goal of the Substrait project is to expose Calcite capabilities more easily to non-jvm technologies as well as expose query planning operations as microservices. Apache Arrow : The Arrow format for data is what the Substrait specification attempts to be for compute expressions. A key goal of Substrait is to enable substrait producers to execute work within the Arrow Rust and C++ compute kernels. Why not use SQL? \u00b6 POSIX SQL is a well known language for describing queries against relational data. It is designed to be simple and allow reading and writing by humans. Substrait is not intended as a replacement for SQL and works alongside SQL to provide capabilities that SQL lacks. SQL is not a great fit for systems that actually satisfy the query because it does not provide sufficient detail and is not represented in a format that is easy for processing. Because of this, most modern systems will first translate the SQL query into a query plan, sometimes called the execution plan. There can be multiple levels of a query plan (e.g. physical and logical), a query plan may be split up and distributed across multiple systems, and a query plan often undergoes simplifying or optimizing transformations. The SQL standard does not define the format of the query or execution plan and there is no open format that is supported by a broad set of systems. Substrait was created to provide a standard and open format for these query plans. Why not just do this within an existing OSS project? \u00b6 A key goal of the Substrait project is to not be coupled to any single existing technology. Trying to get people involved in something can be difficult when it seems to be primarily driven by the opinions and habits of a single community. In many ways, this situation is similar to the early situation with Arrow. The precursor to Arrow was the Apache Drill ValueVectors concepts. As part of creating Arrow, Wes and Jacques recognized the need to create a new community to build a fresh consensus (beyond just what the Apache Drill community wanted). This separation and new independent community was a key ingredient to Arrow\u2019s current success. The needs here are much the same\u2013many separate communities could benefit from Substrait but each have their own pain points, type systems, development processes and timelines. To help resolve these tensions, one of the approaches proposed in Substrait is to set a bar that at least two of the top four OSS data technologies (Arrow, Spark, Iceberg, Trino) supports something before incorporating it directly into the Substrait specification. (Another goal is to support strong extension points at key locations to avoid this bar being a limiter to broad adoption.) Why the name Substrait? \u00b6 A strait is a narrow connector of water between two other pieces of water. In analytics, data is often thought of as water. Substrait is focused on instructions related to the data. In other words, what defines or supports the movement of water between one or more larger systems. Thus the underlayment for the strait connecting different pools of water => sub-strait.","title":"Home"},{"location":"#substrait-cross-language-serialization-for-relational-algebra","text":"","title":"Substrait: Cross-Language Serialization for Relational Algebra"},{"location":"#project-vision","text":"Create a well-defined, cross-language specification for data compute operations. This includes a declaration of common operations, custom operations and one or more serialized representations of this specification. The spec focuses on the semantics of each operation and a consistent way to describe. In many ways, the goal of this project is similar to that of the Apache Arrow project. Arrow is focused on a standardized memory representation of columnar data. Substrait is focused on what should be done to data.","title":"Project Vision"},{"location":"#example-use-cases","text":"Communicate a compute plan between a SQL parser and an execution engine (e.g. Calcite SQL parsing to Arrow C++ compute kernel) Serialize a plan that represents a SQL view for consistent use in multiple systems (e.g. Iceberg views in Spark and Trino) Submit a plan to different execution engines (e.g. Datafusion and Postgres) and get a consistent interpretation of the semantics. Create an alternative plan generation implementation that can connect an existing end-user compute expression system to an existing end-user processing engine (e.g. Pandas operations executed inside SingleStore) Build a pluggable plan visualization tool (e.g. D3 based plan visualizer)","title":"Example Use Cases"},{"location":"#community-principles","text":"Be inclusive and open to all. If you want to join the project, open a PR or issue , start a discussion or join the Slack Channel . Ensure a diverse set of contributors that come from multiple data backgrounds to maximize general utility. Build a specification based on open consensus. Avoid over-reliance/coupling to any single technology. Make the specification and all tools freely available on a permissive license (ApacheV2)","title":"Community Principles"},{"location":"#related-technologies","text":"Apache Calcite : Many ideas in Substrait are inspired by the Calcite project. Calcite is a great JVM-based sql query parsing and optimization framework. A key goal of the Substrait project is to expose Calcite capabilities more easily to non-jvm technologies as well as expose query planning operations as microservices. Apache Arrow : The Arrow format for data is what the Substrait specification attempts to be for compute expressions. A key goal of Substrait is to enable substrait producers to execute work within the Arrow Rust and C++ compute kernels.","title":"Related Technologies"},{"location":"#why-not-use-sql","text":"POSIX SQL is a well known language for describing queries against relational data. It is designed to be simple and allow reading and writing by humans. Substrait is not intended as a replacement for SQL and works alongside SQL to provide capabilities that SQL lacks. SQL is not a great fit for systems that actually satisfy the query because it does not provide sufficient detail and is not represented in a format that is easy for processing. Because of this, most modern systems will first translate the SQL query into a query plan, sometimes called the execution plan. There can be multiple levels of a query plan (e.g. physical and logical), a query plan may be split up and distributed across multiple systems, and a query plan often undergoes simplifying or optimizing transformations. The SQL standard does not define the format of the query or execution plan and there is no open format that is supported by a broad set of systems. Substrait was created to provide a standard and open format for these query plans.","title":"Why not use SQL?"},{"location":"#why-not-just-do-this-within-an-existing-oss-project","text":"A key goal of the Substrait project is to not be coupled to any single existing technology. Trying to get people involved in something can be difficult when it seems to be primarily driven by the opinions and habits of a single community. In many ways, this situation is similar to the early situation with Arrow. The precursor to Arrow was the Apache Drill ValueVectors concepts. As part of creating Arrow, Wes and Jacques recognized the need to create a new community to build a fresh consensus (beyond just what the Apache Drill community wanted). This separation and new independent community was a key ingredient to Arrow\u2019s current success. The needs here are much the same\u2013many separate communities could benefit from Substrait but each have their own pain points, type systems, development processes and timelines. To help resolve these tensions, one of the approaches proposed in Substrait is to set a bar that at least two of the top four OSS data technologies (Arrow, Spark, Iceberg, Trino) supports something before incorporating it directly into the Substrait specification. (Another goal is to support strong extension points at key locations to avoid this bar being a limiter to broad adoption.)","title":"Why not just do this within an existing OSS project?"},{"location":"#why-the-name-substrait","text":"A strait is a narrow connector of water between two other pieces of water. In analytics, data is often thought of as water. Substrait is focused on instructions related to the data. In other words, what defines or supports the movement of water between one or more larger systems. Thus the underlayment for the strait connecting different pools of water => sub-strait.","title":"Why the name Substrait?"},{"location":"community/","text":"Community \u00b6 Substrait is developed as a consensus-driven open source product under the Apache 2.0 license. Development is done in the open leveraging GitHub issues and PRs. Get In Touch \u00b6 Slack Channel The developers on Substrait frequent the Slack. You can get an invite to the channel by following this link . GitHub Issues Substrait is developed via GitHub issues and pull requests. If you see a problem or want to enhance the product, we suggest you file a GitHub issue for developers to review. Twitter The @substrait_io account on Twitter is our official account. Follow-up to keep to date on what is happening with Substrait! Docs Our website is all maintained in our source repository. If there is something you think can be improved, feel free to fork our repository and post a pull request. Reviewable We use Reviewable (in addition to GitHub reviews) to collaborate on code and docs. They are a great service and support OSS projects. Contribution \u00b6 All contributors are welcome to Substrait.","title":"Community"},{"location":"community/#community","text":"Substrait is developed as a consensus-driven open source product under the Apache 2.0 license. Development is done in the open leveraging GitHub issues and PRs.","title":"Community"},{"location":"community/#get-in-touch","text":"Slack Channel The developers on Substrait frequent the Slack. You can get an invite to the channel by following this link . GitHub Issues Substrait is developed via GitHub issues and pull requests. If you see a problem or want to enhance the product, we suggest you file a GitHub issue for developers to review. Twitter The @substrait_io account on Twitter is our official account. Follow-up to keep to date on what is happening with Substrait! Docs Our website is all maintained in our source repository. If there is something you think can be improved, feel free to fork our repository and post a pull request. Reviewable We use Reviewable (in addition to GitHub reviews) to collaborate on code and docs. They are a great service and support OSS projects.","title":"Get In Touch"},{"location":"community/#contribution","text":"All contributors are welcome to Substrait.","title":"Contribution"},{"location":"expressions/aggregate_functions/","text":"Aggregate Functions \u00b6 Aggregate functions are functions that define an operation which consumes values from multiple records to a produce a single output. Aggregate functions in SQL are typically used in GROUP BY functions. Aggregate functions are similar to scalar functions and function signatures with a small set of different properties. Aggregate function signatures contain all of the properties defined for scalar functions . Additionally, they contain the properties below Property Description Required Inherits All properties defined for scalar function N/A Ordered Whether this aggregation function should allow user ordering Optional, defaults to false Maximum set size Maximum allowed set size as an unsigned integer Optional, defaults to unlimited Decomposable Whether the funtion can be executed in one or more intermediate steps. Valid options are: NONE, ONE, MANY, describing how intermediate steps can be taken. Optional, defaults to NONE Intermediate Output Type If the function is decomposable, represents the intermediate output type that is used, if the function is defined as either ONE or MANY decomposable. Will be a struct in many cases. Required for ONE and MANY. Aggregate Binding \u00b6 When Binding an Aggregate function, the binding must include the following additional properties beyond the standard scalar binding properties: Property Description Phase Describes the input type of the data: [INITIAL_TO_INTERMEDIATE, INTERMEDIATE_TO_INTERMEDIATE, INITIAL_TO_RESULT, INTERMEDIATE_TO_RESULT] describing what portion of the operation is required. For functions that are NOT decomposable, the only valid option will be INITIAL_TO_RESULT. Ordering One or more ordering keys along with key order (ASC|DESC|NULL FIRST, etc), declared similar to the sort keys in a order by relational operation. Only allowed in cases where the function signature supports Ordering.","title":"Aggregate Functions"},{"location":"expressions/aggregate_functions/#aggregate-functions","text":"Aggregate functions are functions that define an operation which consumes values from multiple records to a produce a single output. Aggregate functions in SQL are typically used in GROUP BY functions. Aggregate functions are similar to scalar functions and function signatures with a small set of different properties. Aggregate function signatures contain all of the properties defined for scalar functions . Additionally, they contain the properties below Property Description Required Inherits All properties defined for scalar function N/A Ordered Whether this aggregation function should allow user ordering Optional, defaults to false Maximum set size Maximum allowed set size as an unsigned integer Optional, defaults to unlimited Decomposable Whether the funtion can be executed in one or more intermediate steps. Valid options are: NONE, ONE, MANY, describing how intermediate steps can be taken. Optional, defaults to NONE Intermediate Output Type If the function is decomposable, represents the intermediate output type that is used, if the function is defined as either ONE or MANY decomposable. Will be a struct in many cases. Required for ONE and MANY.","title":"Aggregate Functions"},{"location":"expressions/aggregate_functions/#aggregate-binding","text":"When Binding an Aggregate function, the binding must include the following additional properties beyond the standard scalar binding properties: Property Description Phase Describes the input type of the data: [INITIAL_TO_INTERMEDIATE, INTERMEDIATE_TO_INTERMEDIATE, INITIAL_TO_RESULT, INTERMEDIATE_TO_RESULT] describing what portion of the operation is required. For functions that are NOT decomposable, the only valid option will be INITIAL_TO_RESULT. Ordering One or more ordering keys along with key order (ASC|DESC|NULL FIRST, etc), declared similar to the sort keys in a order by relational operation. Only allowed in cases where the function signature supports Ordering.","title":"Aggregate Binding"},{"location":"expressions/embedded_functions/","text":"Embedded Functions \u00b6 Embedded functions are a special kind of function where the implementation is embedded within the actual plan. They are commonly used in tools where a user intersperses business logic within a data pipeline. This is more common in data science workflows than traditional SQL workflows. Embedded functions are not pre-registered. Embedded functions require that data be consumed and produced with a standard API, may require memory allocation and have determinate error reporting behavior. They may also have specific runtime dependencies. For example, a python pickle function may depend on pyarrow 5.0 and pynessie 1.0. Properties for an embedded function include: Property Description Required Function Type The type of embedded function presented Required Function Properties Function properties, one of those items defined below. Required Output Type The fully resolved output type for this embedded function. Required The binary representation of an embedded function is: Binary Representation message EmbeddedFunction { repeated Expression arguments = 1 ; Type output_type = 2 ; oneof kind { PythonPickleFunction python_pickle_function = 3 ; WebAssemblyFunction web_assembly_function = 4 ; } message PythonPickleFunction { bytes function = 1 ; repeated string prerequisite = 2 ; } message WebAssemblyFunction { bytes script = 1 ; repeated string prerequisite = 2 ; } } Human Readable Representation n/a Example n/a Function Details \u00b6 There are many type of types of possible stored functions. For each, Substrait works to expose the function in as descriptive a way as possible to support the largest number of consumers. Python Pickle Function Type \u00b6 Property Description Required Pickle Body binary pickle encoded function using [TBD] api representation to access arguments. True Prereqs A list of specific python conda packages that are prerequisites for access (a structured version of a requirements.txt file) Optional, defaults to none WebAssembly Function Type \u00b6 Property Description Required Script WebAssembly function True Prereqs A list of AssemblyScript prerequisites required to compile the assemblyscript function using NPM coordinates Optional, defaults to none Discussion Points \u00b6 What are the common embedded function formats? How do we expose the data for a function? How do we express batching capabilities? How do we ensure/declare containerization?","title":"Embedded Functions"},{"location":"expressions/embedded_functions/#embedded-functions","text":"Embedded functions are a special kind of function where the implementation is embedded within the actual plan. They are commonly used in tools where a user intersperses business logic within a data pipeline. This is more common in data science workflows than traditional SQL workflows. Embedded functions are not pre-registered. Embedded functions require that data be consumed and produced with a standard API, may require memory allocation and have determinate error reporting behavior. They may also have specific runtime dependencies. For example, a python pickle function may depend on pyarrow 5.0 and pynessie 1.0. Properties for an embedded function include: Property Description Required Function Type The type of embedded function presented Required Function Properties Function properties, one of those items defined below. Required Output Type The fully resolved output type for this embedded function. Required The binary representation of an embedded function is: Binary Representation message EmbeddedFunction { repeated Expression arguments = 1 ; Type output_type = 2 ; oneof kind { PythonPickleFunction python_pickle_function = 3 ; WebAssemblyFunction web_assembly_function = 4 ; } message PythonPickleFunction { bytes function = 1 ; repeated string prerequisite = 2 ; } message WebAssemblyFunction { bytes script = 1 ; repeated string prerequisite = 2 ; } } Human Readable Representation n/a Example n/a","title":"Embedded Functions"},{"location":"expressions/embedded_functions/#function-details","text":"There are many type of types of possible stored functions. For each, Substrait works to expose the function in as descriptive a way as possible to support the largest number of consumers.","title":"Function Details"},{"location":"expressions/embedded_functions/#python-pickle-function-type","text":"Property Description Required Pickle Body binary pickle encoded function using [TBD] api representation to access arguments. True Prereqs A list of specific python conda packages that are prerequisites for access (a structured version of a requirements.txt file) Optional, defaults to none","title":"Python Pickle Function Type"},{"location":"expressions/embedded_functions/#webassembly-function-type","text":"Property Description Required Script WebAssembly function True Prereqs A list of AssemblyScript prerequisites required to compile the assemblyscript function using NPM coordinates Optional, defaults to none","title":"WebAssembly Function Type"},{"location":"expressions/embedded_functions/#discussion-points","text":"What are the common embedded function formats? How do we expose the data for a function? How do we express batching capabilities? How do we ensure/declare containerization?","title":"Discussion Points"},{"location":"expressions/field_references/","text":"Field References \u00b6 In Substrait, all fields are dealt with on a positional basis. Field names are only used at the edge of a plan for the purposes of naming field ids. Each operation returns a simple or compound data type. Additional operations can refer to data within that initial operation using field references. To reference a field, you use a reference based on the type of field position you want to reference. Reference Type Properties Type Applicability Type return Struct Field Ordinal position. Zero-based. Only legal within the range of possible fields within a struct. Selecting an ordinal outside the applicable field range results in an invalid plan. struct Type of field referenced Array Value Array offset. Zero-based. Negative number can be used to describe a offset relative to the end of the array. For example, -1 means the last element in an array. Negative and positive overflows return null values (no wrapping). list type of list Array Slice Array offset and element count. Zero-based. Negative number can be used to describe a offset relative to the end of the array. For example, -1 means the last element in an array. Position does not wrap, nor does length. list Same type as original list Map Key A map value that is matched exactly against available map keys and returned. [TBD, can multiple matches occur?] map Value type of map Map KeyExpression A wildcard string that is matched against a simplified form of regular expressions. Requires the key type of the map to be a character type. [Format detail needed, intention to include basic regex concepts such as greedy/non greedy.] map List of map value type Masked Complex Expression An expression that provides a mask over a schema declaring which portions of the schema should be presented. This allows a user to select a portion of a complex object but mask certain subsections of that same object. any any Compound References \u00b6 References are typically constructed as a sequence. For example: [struct position 0, struct position 1, array offset 2, array slice 1..3]. Validation \u00b6 References must validate against the schema of the record being referenced. If not, an error is expected. Masked Complex Expression \u00b6 A masked complex expression is used to do a subselect of a portion of a complex record. It allows a user to specify the portion of the complex object to consume. Imagine you have a schema of (note that structs are lists fields here, as they are in general in Substrait as field names are not used internally in Substrait): struct: - struct: - integer - list: struct: - i32 - string - string - i32 - i16 - i32 - i64 Given this schema, you could declare a mask in pseudo code such as: 0:[0,1:[..5:[0,2]]],2,3 OR O: - 0 - 1: ..5: -0 -2 1 This mask states that we would like to include fields 0 2 and 3 at the top-level. Within field 0, we want to include subfields 0 and 1. For subfield 0.1, we want to include up to only the first 5 records in the array and only includes fields 0 and 2 within the struct within that array. The resulting schema would be: struct: - struct: - integer - list: struct: - i32 - string - i32 Unwrapping Behavior \u00b6 By default, when only a single field is selected from a struct, that struct is removed. When only a single element is removed from a list, the list is removed. A user can also configure the mask to avoid unwrapping in these cases. [TBD how we express this in the serialization formats.] Discussion Points \u00b6 Should we support column reordering/positioning using a masked complex expression? (Right now, you can only mask things out.)","title":"Field References"},{"location":"expressions/field_references/#field-references","text":"In Substrait, all fields are dealt with on a positional basis. Field names are only used at the edge of a plan for the purposes of naming field ids. Each operation returns a simple or compound data type. Additional operations can refer to data within that initial operation using field references. To reference a field, you use a reference based on the type of field position you want to reference. Reference Type Properties Type Applicability Type return Struct Field Ordinal position. Zero-based. Only legal within the range of possible fields within a struct. Selecting an ordinal outside the applicable field range results in an invalid plan. struct Type of field referenced Array Value Array offset. Zero-based. Negative number can be used to describe a offset relative to the end of the array. For example, -1 means the last element in an array. Negative and positive overflows return null values (no wrapping). list type of list Array Slice Array offset and element count. Zero-based. Negative number can be used to describe a offset relative to the end of the array. For example, -1 means the last element in an array. Position does not wrap, nor does length. list Same type as original list Map Key A map value that is matched exactly against available map keys and returned. [TBD, can multiple matches occur?] map Value type of map Map KeyExpression A wildcard string that is matched against a simplified form of regular expressions. Requires the key type of the map to be a character type. [Format detail needed, intention to include basic regex concepts such as greedy/non greedy.] map List of map value type Masked Complex Expression An expression that provides a mask over a schema declaring which portions of the schema should be presented. This allows a user to select a portion of a complex object but mask certain subsections of that same object. any any","title":"Field References"},{"location":"expressions/field_references/#compound-references","text":"References are typically constructed as a sequence. For example: [struct position 0, struct position 1, array offset 2, array slice 1..3].","title":"Compound References"},{"location":"expressions/field_references/#validation","text":"References must validate against the schema of the record being referenced. If not, an error is expected.","title":"Validation"},{"location":"expressions/field_references/#masked-complex-expression","text":"A masked complex expression is used to do a subselect of a portion of a complex record. It allows a user to specify the portion of the complex object to consume. Imagine you have a schema of (note that structs are lists fields here, as they are in general in Substrait as field names are not used internally in Substrait): struct: - struct: - integer - list: struct: - i32 - string - string - i32 - i16 - i32 - i64 Given this schema, you could declare a mask in pseudo code such as: 0:[0,1:[..5:[0,2]]],2,3 OR O: - 0 - 1: ..5: -0 -2 1 This mask states that we would like to include fields 0 2 and 3 at the top-level. Within field 0, we want to include subfields 0 and 1. For subfield 0.1, we want to include up to only the first 5 records in the array and only includes fields 0 and 2 within the struct within that array. The resulting schema would be: struct: - struct: - integer - list: struct: - i32 - string - i32","title":"Masked Complex Expression"},{"location":"expressions/field_references/#unwrapping-behavior","text":"By default, when only a single field is selected from a struct, that struct is removed. When only a single element is removed from a list, the list is removed. A user can also configure the mask to avoid unwrapping in these cases. [TBD how we express this in the serialization formats.]","title":"Unwrapping Behavior"},{"location":"expressions/field_references/#discussion-points","text":"Should we support column reordering/positioning using a masked complex expression? (Right now, you can only mask things out.)","title":"Discussion Points"},{"location":"expressions/scalar_functions/","text":"Scalar Functions \u00b6 A function is a scalar function if that function takes in values from a single record and produces an output value. To clearly specify the definition of functions, Substrait declares a extensible specification plus binding approach to function resolution. A scalar function signature includes the following properties: Property Description Required Name One or more user friendly utf8 strings that are used to reference this function in languages At least one value is required. List of arguments Argument properties are defined below. Arguments can be fully defined or calculated with a type expression. See further details below. Optional, defaults to niladic. Deterministic Whether this function is expected to reproduce the same output when it is invoked multiple times with the same input. This informs a plan consumer on whether it can constant reduce the defined function. An example would be a random() function, which is typically expected to be evaluated repeatedly despite having the same set of inputs. Optional, defaults to true. Session Dependent Whether this function is influenced by the session context it is invoked within. For example, a function may be influenced by a user who is invoking the function, the time zone of a session, or some other non-obvious parameter. This can inform caching systems on whether a particular function is cacheable. Optional, defaults to false. Variadic Behavior Whether the last argument of the function is variadic or a single argument. If variadic the argument can optionally have a lower bound (minimum number of instances) and upper bound (maximum number of instances) Optional, defaults to single value. Description Additional description of function for implementers or users. Should be written human readable to allow exposure to end users. Presented as a map with language => description mappings. E.g. { \"en\": \"This adds two numbers together.\", \"fr\": \"cela ajoute deux nombres\"} . Optional Return Value The output type of the expression. Return types can be expressed as a fully-defined type or a type expression. See below for more on type expressions. Required Implementation Map A map of implementation locations for one or more implementations of the given function. Each key is a function implementation type. Implementation types include examples such as: AthenaArrowLambda, TrinoV361Jar, ArrowCppKernelEnum, GandivaEnum, LinkedIn Transport Jar, etc. [Definition TBD]. Implementation type has one or more properties associated with retrieval of that implementation. Optional Argument Types \u00b6 There are two main types of arguments: Value Arguments and Type Arguments. Value Arguments: An argument that refers to a data value. This could be a constant (a literal expression defined in the plan) or variable (A reference expression that references data being processed by the plan). This is the most common type of argument. Value arguments are not available in output derivation. Value arguments can be declared in one of two ways: concrete or parameterized. Concrete Types are either simple types or compound types with all parameters fully defined (without referencing any type arguments). Examples include i32, fp32, VARCHAR(20), List<fp32>, etc. Parameterized types are discussed further below. Type Arguments: Type arguments are used to inform the evaluation and/or type derivation of the function. For example, you might have a function which is truncate(<type> DECIMAL(P0,S0), <value> DECIMAL(P1, S1), <value> i32) . This function declares two value arguments and a type argument. The type argument\u2019s value can be used to determine the output type (The types of the values arguments can also be used for output type derivation.) Value Argument Properties \u00b6 Property Description Required Name A human readable name for this argument to help clarify use. Optional, defaults to a name based on position (e.g. arg0) Type A fully defined type or a type expression. Required Constant Whether this argument is required to be a constant for invocation. For example, in some system a regular expression pattern would only be accepted as a literal and not a column value reference. Optional, defaults to false Type Argument Properties \u00b6 Property Description Required Type A partially or completely parameterized type. E.g. List<K> or K Required Name A human readable name for this argument to help clarify use. Optional, defaults to a name based on position (e.g. arg0) Parameterized Types \u00b6 Types are parameterized by two types of values: by inner types (e.g. List<K>) and numeric values (e.g. VARCHAR(P,S)). Parameter names are simple strings (frequently a single character). There are two types of parameters: integer parameters and type parameters. When the same parameter name is used multiple times in a function definition, the function can only bind if the exact same value is used for all parameters of that name. For example, if one had a function with a signature of `fn(VARCHAR(N), VARCHAR(N)), the function would be only be usable if both VARCHAR types had the same length value N. This necessitates that all instances of the same parameter name must be of the same parameter type (all instances are a type parameter or all instances are an integer parameter). Integer Parameter Bounds \u00b6 A type which declares one or more integer parameters can also declare a range of legal values for a function. For example, one could declare a function fn(VARCHAR(N)) where N IN [0..20] to describe a function which only takes in varchar types that are less than or equal to 20 characters in length. Type Parameter Bounds \u00b6 A parameterized type can either be unbounded (any type is allowed) or be bounded. When bounded, the function can only be bound to arguments of the types bound to. For example, one could declare a function fn(Map<K,V> map) where K IN [STRING, VARCHAR(N), FIXEDCHAR(N)] which would allow the function to accept a map argument as long as the key type is one of STRING, VARCHAR or FIXEDCHAR. Note, when duplicate parameter names are used in disjunct bounds options, the names will use the same bounds. For example, in the above map function, if one were to declare a bound such as where N IN [0..20], this limit would apply to either N parameter. Type Parameter Resolution in variadic functions \u00b6 When the last argument of a function is variadic and declares a type parameter e.g. fn(A, B, C...) , the C parameter can be marked as either consistent or inconsistent. If marked as consistent, the function can only be bound to arguments where all of the C types are the same concrete type. If marked as inconsistent, each unique C can be bound to a different type within the constraints of what T allows. Output Type Derivation \u00b6 Concrete Return Types \u00b6 A concrete return type is one that is fully known at function definition time. Example simple concrete return types would be things such as i32, fp32. For compound types, a concrete return type must be fully declared. Example of fully defined compound types: VARCHAR(20), DECIMAL(25,5) Return Type Expressions \u00b6 Any function can declare a return type expression. A return type expression uses a simplified set of expressions to describe how the return type should be returned. For example, a return expression could be as simple as the return of parameter declared in the arguments. For example f(List<K>) =>K or can be a simple mathematical or conditional expression such as add(decimal(a,b), decimal(c,d)) => decimal(a+c, b+d) . For the simple expression language, there is a very narrow set of types: Integer: 64 bit signed integer (can be a literal or a parameter value) Boolean: True and False Type: A Substrait type (with possibly additional embedded expressions) These types are evaluated using a small set of operations to support common scenarios. List of valid operations: Math: +, -, *, /, min, max Boolean: &&, ||, !, <, >, == Parameters: type, integer Literals: type, integer Fully defined with argument types: type_parameter(string name) => type integer_parameter(string name) => integer not(boolean x) => boolean and(boolean a, boolean b) => boolean or(boolean a, boolean b) => boolean multiply(integer a, integer b) => integer divide(integer a, integer b) => integer add(integer a, integer b) => integer subtract(integer a, integer b) => integer min(integer a, integer b) => integer max(integer a, integer b) => integer equal(integer a, integer b) => boolean greater_than(integer a, integer b) => boolean less_than(integer a, integer b) => boolean covers(Type a, Type b) => boolean Covers means that type b matches type A for as much as type B is defined. For example, if type A is VARCHAR(20) and type B is VARCHAR(N) , type B would be considered covering. Similarlily if type A was List<Struct<a:f32, b:f32>> and type B was List<Struct<>> , it would be considered covering. Note that this is directional \u201cas in B covers A\u201d or \u201cB can be further enhanced to match the definition A\u201d. if(boolean a) then (integer) else (integer) if(boolean a) then (type) else (type) Example Type Expressions \u00b6 For reference, here are are some common output type derivations and how they can be expressed with a return type expression: Operation Definition Add item to list add(<List<T>, T>) => List<T> Decimal Division divide(Decimal(P1,S1), Decimal(P2,S2)) => Decimal(P1 -S1 + S2 + MAX(6, S1 + P2 + 1), MAX(6, S1 + P2 + 1)) Select a subset of map keys based on a regular expression (requires stringlike keys) extract_values(regex:string, map:Map<K,V>) => List<V> WHERE K IN [STRING, VARCHAR(N), FIXEDCHAR(N)] Concatenate two fixed sized character strings concat(FIXEDCHAR(A), FIXEDCHAR(B)) => FIXEDCHAR(A+B) Make a struct of a set of fields and a struct definition. make_struct(<type> T, K...) => T","title":"Scalar Functions"},{"location":"expressions/scalar_functions/#scalar-functions","text":"A function is a scalar function if that function takes in values from a single record and produces an output value. To clearly specify the definition of functions, Substrait declares a extensible specification plus binding approach to function resolution. A scalar function signature includes the following properties: Property Description Required Name One or more user friendly utf8 strings that are used to reference this function in languages At least one value is required. List of arguments Argument properties are defined below. Arguments can be fully defined or calculated with a type expression. See further details below. Optional, defaults to niladic. Deterministic Whether this function is expected to reproduce the same output when it is invoked multiple times with the same input. This informs a plan consumer on whether it can constant reduce the defined function. An example would be a random() function, which is typically expected to be evaluated repeatedly despite having the same set of inputs. Optional, defaults to true. Session Dependent Whether this function is influenced by the session context it is invoked within. For example, a function may be influenced by a user who is invoking the function, the time zone of a session, or some other non-obvious parameter. This can inform caching systems on whether a particular function is cacheable. Optional, defaults to false. Variadic Behavior Whether the last argument of the function is variadic or a single argument. If variadic the argument can optionally have a lower bound (minimum number of instances) and upper bound (maximum number of instances) Optional, defaults to single value. Description Additional description of function for implementers or users. Should be written human readable to allow exposure to end users. Presented as a map with language => description mappings. E.g. { \"en\": \"This adds two numbers together.\", \"fr\": \"cela ajoute deux nombres\"} . Optional Return Value The output type of the expression. Return types can be expressed as a fully-defined type or a type expression. See below for more on type expressions. Required Implementation Map A map of implementation locations for one or more implementations of the given function. Each key is a function implementation type. Implementation types include examples such as: AthenaArrowLambda, TrinoV361Jar, ArrowCppKernelEnum, GandivaEnum, LinkedIn Transport Jar, etc. [Definition TBD]. Implementation type has one or more properties associated with retrieval of that implementation. Optional","title":"Scalar Functions"},{"location":"expressions/scalar_functions/#argument-types","text":"There are two main types of arguments: Value Arguments and Type Arguments. Value Arguments: An argument that refers to a data value. This could be a constant (a literal expression defined in the plan) or variable (A reference expression that references data being processed by the plan). This is the most common type of argument. Value arguments are not available in output derivation. Value arguments can be declared in one of two ways: concrete or parameterized. Concrete Types are either simple types or compound types with all parameters fully defined (without referencing any type arguments). Examples include i32, fp32, VARCHAR(20), List<fp32>, etc. Parameterized types are discussed further below. Type Arguments: Type arguments are used to inform the evaluation and/or type derivation of the function. For example, you might have a function which is truncate(<type> DECIMAL(P0,S0), <value> DECIMAL(P1, S1), <value> i32) . This function declares two value arguments and a type argument. The type argument\u2019s value can be used to determine the output type (The types of the values arguments can also be used for output type derivation.)","title":"Argument Types"},{"location":"expressions/scalar_functions/#value-argument-properties","text":"Property Description Required Name A human readable name for this argument to help clarify use. Optional, defaults to a name based on position (e.g. arg0) Type A fully defined type or a type expression. Required Constant Whether this argument is required to be a constant for invocation. For example, in some system a regular expression pattern would only be accepted as a literal and not a column value reference. Optional, defaults to false","title":"Value Argument Properties"},{"location":"expressions/scalar_functions/#type-argument-properties","text":"Property Description Required Type A partially or completely parameterized type. E.g. List<K> or K Required Name A human readable name for this argument to help clarify use. Optional, defaults to a name based on position (e.g. arg0)","title":"Type Argument Properties"},{"location":"expressions/scalar_functions/#parameterized-types","text":"Types are parameterized by two types of values: by inner types (e.g. List<K>) and numeric values (e.g. VARCHAR(P,S)). Parameter names are simple strings (frequently a single character). There are two types of parameters: integer parameters and type parameters. When the same parameter name is used multiple times in a function definition, the function can only bind if the exact same value is used for all parameters of that name. For example, if one had a function with a signature of `fn(VARCHAR(N), VARCHAR(N)), the function would be only be usable if both VARCHAR types had the same length value N. This necessitates that all instances of the same parameter name must be of the same parameter type (all instances are a type parameter or all instances are an integer parameter).","title":"Parameterized Types"},{"location":"expressions/scalar_functions/#integer-parameter-bounds","text":"A type which declares one or more integer parameters can also declare a range of legal values for a function. For example, one could declare a function fn(VARCHAR(N)) where N IN [0..20] to describe a function which only takes in varchar types that are less than or equal to 20 characters in length.","title":"Integer Parameter Bounds"},{"location":"expressions/scalar_functions/#type-parameter-bounds","text":"A parameterized type can either be unbounded (any type is allowed) or be bounded. When bounded, the function can only be bound to arguments of the types bound to. For example, one could declare a function fn(Map<K,V> map) where K IN [STRING, VARCHAR(N), FIXEDCHAR(N)] which would allow the function to accept a map argument as long as the key type is one of STRING, VARCHAR or FIXEDCHAR. Note, when duplicate parameter names are used in disjunct bounds options, the names will use the same bounds. For example, in the above map function, if one were to declare a bound such as where N IN [0..20], this limit would apply to either N parameter.","title":"Type Parameter Bounds"},{"location":"expressions/scalar_functions/#type-parameter-resolution-in-variadic-functions","text":"When the last argument of a function is variadic and declares a type parameter e.g. fn(A, B, C...) , the C parameter can be marked as either consistent or inconsistent. If marked as consistent, the function can only be bound to arguments where all of the C types are the same concrete type. If marked as inconsistent, each unique C can be bound to a different type within the constraints of what T allows.","title":"Type Parameter Resolution in variadic functions"},{"location":"expressions/scalar_functions/#output-type-derivation","text":"","title":"Output Type Derivation"},{"location":"expressions/scalar_functions/#concrete-return-types","text":"A concrete return type is one that is fully known at function definition time. Example simple concrete return types would be things such as i32, fp32. For compound types, a concrete return type must be fully declared. Example of fully defined compound types: VARCHAR(20), DECIMAL(25,5)","title":"Concrete Return Types"},{"location":"expressions/scalar_functions/#return-type-expressions","text":"Any function can declare a return type expression. A return type expression uses a simplified set of expressions to describe how the return type should be returned. For example, a return expression could be as simple as the return of parameter declared in the arguments. For example f(List<K>) =>K or can be a simple mathematical or conditional expression such as add(decimal(a,b), decimal(c,d)) => decimal(a+c, b+d) . For the simple expression language, there is a very narrow set of types: Integer: 64 bit signed integer (can be a literal or a parameter value) Boolean: True and False Type: A Substrait type (with possibly additional embedded expressions) These types are evaluated using a small set of operations to support common scenarios. List of valid operations: Math: +, -, *, /, min, max Boolean: &&, ||, !, <, >, == Parameters: type, integer Literals: type, integer Fully defined with argument types: type_parameter(string name) => type integer_parameter(string name) => integer not(boolean x) => boolean and(boolean a, boolean b) => boolean or(boolean a, boolean b) => boolean multiply(integer a, integer b) => integer divide(integer a, integer b) => integer add(integer a, integer b) => integer subtract(integer a, integer b) => integer min(integer a, integer b) => integer max(integer a, integer b) => integer equal(integer a, integer b) => boolean greater_than(integer a, integer b) => boolean less_than(integer a, integer b) => boolean covers(Type a, Type b) => boolean Covers means that type b matches type A for as much as type B is defined. For example, if type A is VARCHAR(20) and type B is VARCHAR(N) , type B would be considered covering. Similarlily if type A was List<Struct<a:f32, b:f32>> and type B was List<Struct<>> , it would be considered covering. Note that this is directional \u201cas in B covers A\u201d or \u201cB can be further enhanced to match the definition A\u201d. if(boolean a) then (integer) else (integer) if(boolean a) then (type) else (type)","title":"Return Type Expressions"},{"location":"expressions/scalar_functions/#example-type-expressions","text":"For reference, here are are some common output type derivations and how they can be expressed with a return type expression: Operation Definition Add item to list add(<List<T>, T>) => List<T> Decimal Division divide(Decimal(P1,S1), Decimal(P2,S2)) => Decimal(P1 -S1 + S2 + MAX(6, S1 + P2 + 1), MAX(6, S1 + P2 + 1)) Select a subset of map keys based on a regular expression (requires stringlike keys) extract_values(regex:string, map:Map<K,V>) => List<V> WHERE K IN [STRING, VARCHAR(N), FIXEDCHAR(N)] Concatenate two fixed sized character strings concat(FIXEDCHAR(A), FIXEDCHAR(B)) => FIXEDCHAR(A+B) Make a struct of a set of fields and a struct definition. make_struct(<type> T, K...) => T","title":"Example Type Expressions"},{"location":"expressions/specialized_record_expressions/","text":"Specialized Record Expressions \u00b6 While most all types of operations could be reduced to functions, in some cases this would be overly simplistic. Instead, it is helpful to construct some other expression constructs. These constructs should be focused on different expression types as opposed to something that directly related to syntantic sugar. For example, CAST and EXTRACT or SQL operations that are presented using specialized syntax. However, they can easily modeled using a function paradigm with minimal complexity. Literal Expressions \u00b6 For each data type, it is possible to create a literal value for that data type. The representation depends on the serialization format. If Expression \u00b6 An if value expression is an expression composed of one if clause, zero or more else if clauses and an else clause. In pseudo code, they are envisioned as: if <boolean expression> then <result expression 1> else if <boolean expression> then <result expression 2> (zero or more times) else <result expression 3> When an if expression is declared, all return expressions must be the same identical type. Shortcut Behavior \u00b6 An if expression is expected to logically short-cicuit on a postive outcome. This means that a skipped else/elseif expression cannot cause an error. For example, this should not actually throw an error despite the fact that the cast operation should fail. if 'value' = 'value' then 0 else cast('hello' as integer) Switch Expression \u00b6 Switch expression allow a selection of alternate branches based on the value of a given expression. They are an optimized form of a generic if expression where all conditions are equality to the same value. In pseudo-code: switch(value) <value 1> => <return 1> (1 or more times) <else> => <return 3> Return values for a switch expression must all be of identical type. Shortcut Behavior \u00b6 As in if expressions, switch expression evaluation should not be interrupted by \u201croads not taken\u201d. Or List Equality Expression \u00b6 A specialized structure that is often used is a large list of possible values. In SQL, these are typically large IN lists. They can composed of one or more fields. There are two common patterns, single value and multi value. In pseudo code they are represented as: Single Value: expression, [<value1>, <value2>, ... <valueN>] Multi Value: [expressionA, expressionB], [[value1a, value1b], [value2a, value2b].. [valueNa, valueNb]] For single value expressions, these are a compact equivalent of expression = value1 OR expression = value2 OR .. OR expression = valueN . When using an expression of this type, two things are required. The types of the test expression and all value expressions that are related must be of the same type. Additionally, a function signature for equality must be available for the expression type used.","title":"Specialized Record Expressions"},{"location":"expressions/specialized_record_expressions/#specialized-record-expressions","text":"While most all types of operations could be reduced to functions, in some cases this would be overly simplistic. Instead, it is helpful to construct some other expression constructs. These constructs should be focused on different expression types as opposed to something that directly related to syntantic sugar. For example, CAST and EXTRACT or SQL operations that are presented using specialized syntax. However, they can easily modeled using a function paradigm with minimal complexity.","title":"Specialized Record Expressions"},{"location":"expressions/specialized_record_expressions/#literal-expressions","text":"For each data type, it is possible to create a literal value for that data type. The representation depends on the serialization format.","title":"Literal Expressions"},{"location":"expressions/specialized_record_expressions/#if-expression","text":"An if value expression is an expression composed of one if clause, zero or more else if clauses and an else clause. In pseudo code, they are envisioned as: if <boolean expression> then <result expression 1> else if <boolean expression> then <result expression 2> (zero or more times) else <result expression 3> When an if expression is declared, all return expressions must be the same identical type.","title":"If Expression"},{"location":"expressions/specialized_record_expressions/#shortcut-behavior","text":"An if expression is expected to logically short-cicuit on a postive outcome. This means that a skipped else/elseif expression cannot cause an error. For example, this should not actually throw an error despite the fact that the cast operation should fail. if 'value' = 'value' then 0 else cast('hello' as integer)","title":"Shortcut Behavior"},{"location":"expressions/specialized_record_expressions/#switch-expression","text":"Switch expression allow a selection of alternate branches based on the value of a given expression. They are an optimized form of a generic if expression where all conditions are equality to the same value. In pseudo-code: switch(value) <value 1> => <return 1> (1 or more times) <else> => <return 3> Return values for a switch expression must all be of identical type.","title":"Switch Expression"},{"location":"expressions/specialized_record_expressions/#shortcut-behavior_1","text":"As in if expressions, switch expression evaluation should not be interrupted by \u201croads not taken\u201d.","title":"Shortcut Behavior"},{"location":"expressions/specialized_record_expressions/#or-list-equality-expression","text":"A specialized structure that is often used is a large list of possible values. In SQL, these are typically large IN lists. They can composed of one or more fields. There are two common patterns, single value and multi value. In pseudo code they are represented as: Single Value: expression, [<value1>, <value2>, ... <valueN>] Multi Value: [expressionA, expressionB], [[value1a, value1b], [value2a, value2b].. [valueNa, valueNb]] For single value expressions, these are a compact equivalent of expression = value1 OR expression = value2 OR .. OR expression = valueN . When using an expression of this type, two things are required. The types of the test expression and all value expressions that are related must be of the same type. Additionally, a function signature for equality must be available for the expression type used.","title":"Or List Equality Expression"},{"location":"expressions/table_functions/","text":"Table Functions \u00b6 Table functions produce zero or more records for each input record. Table functions use a signature similar to scalar functions. However, they are not allowed in the same contexts. to be completed\u2026","title":"Table Functions"},{"location":"expressions/table_functions/#table-functions","text":"Table functions produce zero or more records for each input record. Table functions use a signature similar to scalar functions. However, they are not allowed in the same contexts. to be completed\u2026","title":"Table Functions"},{"location":"expressions/user_defined_functions/","text":"User Defined Functions \u00b6 Substrait supports the creation of custom functions using the function signature facility described in scalar functions . If a user wants to declare their own custom functions, they can do either of the following: Expose their function signatures publically using a public organization id. Public organization ids are between 0 and 2B (exclusive) and are registered with the Substrait repository here . Define one or more private organization ids. Private organization ids are 2B and above. Public organizations should be automatically mapped by tools, private organizations will have to be manually mapped. Once a organization id is defined and mapped, a plan will validate against the function signatures listed in the extensions table of contents file. Example file . Once a function signature is defined, user defined functions are treated exactly the same as normal functions within the plan.","title":"User Defined Functions"},{"location":"expressions/user_defined_functions/#user-defined-functions","text":"Substrait supports the creation of custom functions using the function signature facility described in scalar functions . If a user wants to declare their own custom functions, they can do either of the following: Expose their function signatures publically using a public organization id. Public organization ids are between 0 and 2B (exclusive) and are registered with the Substrait repository here . Define one or more private organization ids. Private organization ids are 2B and above. Public organizations should be automatically mapped by tools, private organizations will have to be manually mapped. Once a organization id is defined and mapped, a plan will validate against the function signatures listed in the extensions table of contents file. Example file . Once a function signature is defined, user defined functions are treated exactly the same as normal functions within the plan.","title":"User Defined Functions"},{"location":"expressions/window_functions/","text":"Window Functions \u00b6 Window functions are functions that define an operation which consumes values from multiple records to a produce a single output. They are similar to aggregate functions but also have a focused window of analysis to compare to their partition window. The function similar to scalar values to an end user, producing a single value for each input record. However, their consumption visibility for each record production can be many records. Window function signatures contain all of the properties defined for aggregate functions . Additionally, they contain the properties below Property Description Required Inherits All properties defined for aggregate functions N/A Window Type STREAMING or PARTITION. Describes whether the function needs to see all data for the specific partition operation simultaneously. Operations like SUM can produce values in a streaming manner with no complete visibility of the partition. NTILE requires visibility of the entire partition before it can start producing values. Optional, defaults to PARTITION When Binding an Aggregate function, the binding must include the following additional properties beyond the standard scalar binding properties: Property Description Required Partition A list of partitioning expressions. False, defaults to a single partition for the entire dataset Lower Bound Bound Following(int64), Bound Trailing(int64) or CurrentRow False, defaults to start of partition Upper Bound Bound Following(int64), Bound Trailing(int64) or CurrentRow False, defaults to end of partition","title":"Window Functions"},{"location":"expressions/window_functions/#window-functions","text":"Window functions are functions that define an operation which consumes values from multiple records to a produce a single output. They are similar to aggregate functions but also have a focused window of analysis to compare to their partition window. The function similar to scalar values to an end user, producing a single value for each input record. However, their consumption visibility for each record production can be many records. Window function signatures contain all of the properties defined for aggregate functions . Additionally, they contain the properties below Property Description Required Inherits All properties defined for aggregate functions N/A Window Type STREAMING or PARTITION. Describes whether the function needs to see all data for the specific partition operation simultaneously. Operations like SUM can produce values in a streaming manner with no complete visibility of the partition. NTILE requires visibility of the entire partition before it can start producing values. Optional, defaults to PARTITION When Binding an Aggregate function, the binding must include the following additional properties beyond the standard scalar binding properties: Property Description Required Partition A list of partitioning expressions. False, defaults to a single partition for the entire dataset Lower Bound Bound Following(int64), Bound Trailing(int64) or CurrentRow False, defaults to start of partition Upper Bound Bound Following(int64), Bound Trailing(int64) or CurrentRow False, defaults to end of partition","title":"Window Functions"},{"location":"relations/basics/","text":"Basics \u00b6 Substrait is designed to allow a user to construct an arbitrarily complex data transformation plan. The plan is composed of one or more relational operations. Relational operations are well-defined transformation operations that work by taking zero or more input datasets and transforming them into zero or more output transformations. Substrait defines a core set of transformations and users are also able to extend the operations with their own specialized operations. Each relational operation is composed of several different properties. Common properties for relational operations include the following: Property Description Type Emit The set of columns output from this operation and the order of those columns. Logical & Physical Hints A set of optionally provided, optionally consumed information about an operation that better inform execution. These might include estimated number of input and output records, estimated record size, likely filter reduction, estimated dictionary size, etc. These can also include implementation specific pieces of execution information. Physical Constraint A set of runtime constraints around the operation, limiting its consumption based on irl resources (CPU, memory) as well as virtual resources like number of records produced, largest record size, etc. Physical Relational Signatures \u00b6 In functions, function signatures are declared externally to the use of those signatures (function bindings). In the case of relational operations, signatures are declared directly in the specification. This is due to the speed of change and number of total operations. Relational operations in the specification are expected to be <100 for several years with additions being infrequent. On the other hand, there is an expectation of both a much larger number of functions (1,000s) and a much higher velocity of additions. Each Relational Operation must declare the following: Transformation logic around properties of the data. For example, does a relational operation maintain sortedness of a field? Does an operation change the distribution of data? How many input sets does an operation produce? Does the operator produce an output (by specification, we limit relational operations to a single output at this time) What is the schema and field ordering of an output (see emit below)? Emit: Output Ordering \u00b6 A relational operation uses field references to access specific fields of the input stream. Field references are always ordinal based on the order of the incoming streams. Each relational operation must declare the order of its output data. To simpify things, each relational operation can be in one of two modes: Direct output : The order of outputs is based on the definition declared by the relational operation. Remap : A listed ordering of the direct outputs. This remapping can be also used to drop columns no longer used (such as a filter field or join keys after a join). Note that remapping/exclusion can only be done at the outputs root struct. Filtering of compound values or extracting subsets must be done through other operation types (e.g. projection). Relation Properties \u00b6 There are number of predefined properties that exist in Substrait relations. These include: Distribution \u00b6 When data is partitioned across multiple sibling sets, distribution describes that set of properties that apply to any one partition. This is based on a set of distribution expression properties. A distribution is declared as a set of one or more fields and a distribution type across all fields. Property Description Required Distribution Fields List of fields references that describe distribution (e.g. [0,2:4,5:0:0]). The order of these references do not impact results. Required for Partitioned distribution type. Disallowed for Singleton distribution type. Distribution Type PARTITIONED: For a discrete tuple of values for the declared distribution fields, all records with that tuple are located in the same partition. SINGLETON: there will only be a single partition for this operation. Required Orderedness \u00b6 A guarantee that data output from this operation is provided with a sort order. The sort order will be declared based on a set of sort field definitions based on the emitted output of this operation. Property Description Required Sort Fields A list of fields that the data are ordered by. The list is in order of the sort. If sort by [0,1] this means we only consider the data for field 1 is only ordered within each discrete value of field 0. At least one required. Per - Sort Field A field reference that the data is sorted by. Required Per - Sort Type The order of the data. See ordering types below. Required Ordering Types \u00b6 Ordering Descriptions Nulls Position Ascending Returns data in a ascending order based on the quality function associated with the type. Nulls are included before any values. First Descending Returns data in a descending order based on the quality function associated with the type. Nulls are included before any values. First Ascending Returns data in a ascending order based on the quality function associated with the type. Nulls are included after any values. Last Descending Returns data in a descending order based on the quality function associated with the type. Nulls are included after any values. Last Custom function identifier Returns data using a custom function that returns -1, 0, or 1 depending on the order of the data. Per Function Clustered Ensures that all equal values are coalesced (but no ordering betwixt values is defined). E.g, for values 1,2,3,1,2,3, output bould be any of the following 1,1,2,2,3,3 or 2,2,3,3,1,1 or 3,3,2,2,1,1 or 3,3,1,1,2,2. N/A, may appear anywhere but will be coalesced. Discussion Points \u00b6 Do we try to make read definition types more extensible ala function signatures? Is that necessary if we have a custom relational operator? How do we express decomposed types. For example, the Iceberg type above is for early logical planning. Once we do some operations, it may produce a list of Iceberg file reads. This is likely a secondary type of object. We currently include a generic properties property on read type. Do we want this dumping ground?","title":"Basics"},{"location":"relations/basics/#basics","text":"Substrait is designed to allow a user to construct an arbitrarily complex data transformation plan. The plan is composed of one or more relational operations. Relational operations are well-defined transformation operations that work by taking zero or more input datasets and transforming them into zero or more output transformations. Substrait defines a core set of transformations and users are also able to extend the operations with their own specialized operations. Each relational operation is composed of several different properties. Common properties for relational operations include the following: Property Description Type Emit The set of columns output from this operation and the order of those columns. Logical & Physical Hints A set of optionally provided, optionally consumed information about an operation that better inform execution. These might include estimated number of input and output records, estimated record size, likely filter reduction, estimated dictionary size, etc. These can also include implementation specific pieces of execution information. Physical Constraint A set of runtime constraints around the operation, limiting its consumption based on irl resources (CPU, memory) as well as virtual resources like number of records produced, largest record size, etc. Physical","title":"Basics"},{"location":"relations/basics/#relational-signatures","text":"In functions, function signatures are declared externally to the use of those signatures (function bindings). In the case of relational operations, signatures are declared directly in the specification. This is due to the speed of change and number of total operations. Relational operations in the specification are expected to be <100 for several years with additions being infrequent. On the other hand, there is an expectation of both a much larger number of functions (1,000s) and a much higher velocity of additions. Each Relational Operation must declare the following: Transformation logic around properties of the data. For example, does a relational operation maintain sortedness of a field? Does an operation change the distribution of data? How many input sets does an operation produce? Does the operator produce an output (by specification, we limit relational operations to a single output at this time) What is the schema and field ordering of an output (see emit below)?","title":"Relational Signatures"},{"location":"relations/basics/#emit-output-ordering","text":"A relational operation uses field references to access specific fields of the input stream. Field references are always ordinal based on the order of the incoming streams. Each relational operation must declare the order of its output data. To simpify things, each relational operation can be in one of two modes: Direct output : The order of outputs is based on the definition declared by the relational operation. Remap : A listed ordering of the direct outputs. This remapping can be also used to drop columns no longer used (such as a filter field or join keys after a join). Note that remapping/exclusion can only be done at the outputs root struct. Filtering of compound values or extracting subsets must be done through other operation types (e.g. projection).","title":"Emit: Output Ordering"},{"location":"relations/basics/#relation-properties","text":"There are number of predefined properties that exist in Substrait relations. These include:","title":"Relation Properties"},{"location":"relations/basics/#distribution","text":"When data is partitioned across multiple sibling sets, distribution describes that set of properties that apply to any one partition. This is based on a set of distribution expression properties. A distribution is declared as a set of one or more fields and a distribution type across all fields. Property Description Required Distribution Fields List of fields references that describe distribution (e.g. [0,2:4,5:0:0]). The order of these references do not impact results. Required for Partitioned distribution type. Disallowed for Singleton distribution type. Distribution Type PARTITIONED: For a discrete tuple of values for the declared distribution fields, all records with that tuple are located in the same partition. SINGLETON: there will only be a single partition for this operation. Required","title":"Distribution"},{"location":"relations/basics/#orderedness","text":"A guarantee that data output from this operation is provided with a sort order. The sort order will be declared based on a set of sort field definitions based on the emitted output of this operation. Property Description Required Sort Fields A list of fields that the data are ordered by. The list is in order of the sort. If sort by [0,1] this means we only consider the data for field 1 is only ordered within each discrete value of field 0. At least one required. Per - Sort Field A field reference that the data is sorted by. Required Per - Sort Type The order of the data. See ordering types below. Required","title":"Orderedness"},{"location":"relations/basics/#ordering-types","text":"Ordering Descriptions Nulls Position Ascending Returns data in a ascending order based on the quality function associated with the type. Nulls are included before any values. First Descending Returns data in a descending order based on the quality function associated with the type. Nulls are included before any values. First Ascending Returns data in a ascending order based on the quality function associated with the type. Nulls are included after any values. Last Descending Returns data in a descending order based on the quality function associated with the type. Nulls are included after any values. Last Custom function identifier Returns data using a custom function that returns -1, 0, or 1 depending on the order of the data. Per Function Clustered Ensures that all equal values are coalesced (but no ordering betwixt values is defined). E.g, for values 1,2,3,1,2,3, output bould be any of the following 1,1,2,2,3,3 or 2,2,3,3,1,1 or 3,3,2,2,1,1 or 3,3,1,1,2,2. N/A, may appear anywhere but will be coalesced.","title":"Ordering Types"},{"location":"relations/basics/#discussion-points","text":"Do we try to make read definition types more extensible ala function signatures? Is that necessary if we have a custom relational operator? How do we express decomposed types. For example, the Iceberg type above is for early logical planning. Once we do some operations, it may produce a list of Iceberg file reads. This is likely a secondary type of object. We currently include a generic properties property on read type. Do we want this dumping ground?","title":"Discussion Points"},{"location":"relations/embedded_relations/","text":"Embedded Relations \u00b6 Pending. Embedded relations allow a substrait producer to define a set operation that will be embedded in the plan. TODO: define lots of details about what interfaces, languages, formats, etc. Should reasonably be an extension of embedded user defined table functions.","title":"Embedded Relations"},{"location":"relations/embedded_relations/#embedded-relations","text":"Pending. Embedded relations allow a substrait producer to define a set operation that will be embedded in the plan. TODO: define lots of details about what interfaces, languages, formats, etc. Should reasonably be an extension of embedded user defined table functions.","title":"Embedded Relations"},{"location":"relations/logical_relations/","text":"Logical Relations \u00b6 Read Operator \u00b6 The read operator is an operator that produces one output. A simple example would be the reading of a Parquet file. It is expected that many types of reads will be added over time Signature Value Inputs 0 Outputs 1 Property Maintenance N/A (no inputs) Direct Output Order Defaults to the schema of the data read after the optional projection (masked complex expression) is applied. Read Properties \u00b6 Property Description Required Definition The contents of the read property definition. Required Direct Schema Defines the schema of the output of the read (before any emit remapping/hiding). Required Filter A boolean Substrait expression that describes the filter of a iceberg dataset. TBD: define how field referencing works. Optional, defaults to none. Projection A masked complex expression describing the portions of the content that should be read Optional, defaults to all of schema Output properties Declaration of orderedness and/or distribution properties this read produces Optional, defaults to no properties. Properties A list of name/value pairs associated with the read Optional, defaults to empty Read Definition Types \u00b6 Read definition types are built by the community and added to the specification. This is a portion of specification that is expected to grow rapidly. Virtual Table \u00b6 Property Description Required Data Required Required Files Type \u00b6 Property Description Required Items An array Items (path or path glob) associated with the read Required Format per item Enumeration of available formats. Only current option is PARQUET. Required Filter Operation \u00b6 The filter operator eliminates one or more records from the input data based on a boolean filter expression. Signature Value Inputs 1 Outputs 1 Property Maintenance Orderedness, Distribution, remapped by emit Direct Output Order The field order as the input. Filter Properties \u00b6 Property Description Required Input The relational input Required Expression A boolean expression which describes which records are included/excluded. Required Sort Operation \u00b6 The sort operator reorders a dataset based on one or more identified sort fields as well as a sorting function. Signature Value Inputs 1 Outputs 1 Property Maintenance Will update orderedness property to the output of the sort operation. Distribution property only remapped based on emit. Direct Output Order The field order of the input. Sort Properties \u00b6 Property Description Required Input The relational input Required Sort Fields List of one or more fields to sort by. Uses the same properties as the orderedness property. One sort field required Project Operation \u00b6 The project operation will produce one or more additional expressions based on the inputs of the dataset. Signature Value Inputs 1 Outputs 1 Property Maintenance Distribution maintained, mapped by emit. Orderedness: Maintained if no window operations. Extended to include projection fields if fields are direct references. If window operations are present, no orderedness is maintained. Direct Output Order The field order of the input + the list of new expressions in the order they are declared in the expressions list. Project Properties \u00b6 Property Description Required Input The relational input Required Expressions List of one or more expressions to add to the input. At least one expression required Join Operation \u00b6 The join operation will combine two separate inputs into a single output, based on a join expression. A common subtype of joins is a equality join where the join expression is constrained to a list of equality (or equality + null equality) conditions between the two inputs of the join. Signature Value Inputs 2 Outputs 1 Property Maintenance Distribution is maintained. Orderedness is empty post operation. Physical relations may provide better property maintenance. Direct Output Order The emit order of the left input followed by the emit order of the right input. Join Properties \u00b6 Property Description Required Left Input A relational input. Required Right Input A relational input. Required Join Expression A boolean condition that describes whether each record from the left set \u201cmatch\u201d the record from the right set. Field references correspond to the direct output order of the data. Required. Can be the literal True. Join Type One of the join types defined below. Required Join Types \u00b6 Type Description Inner Return records from the left side only if they match the right side. Return records from the right side only when they match the left side. For each cross input match, return a record including the data from both sides. Non-matching records are ignored. Outer Return all records from both the left and right inputs. For each cross input match, return a record including the data from both sides. For any remaining non-match records, return the record from the corresponding input along with nulls for the opposite input. Left Return all records from the left input. For each cross input match, return a record including the data from both sides. For any remaining non-matching records from the left input, return the left record along with nulls for the right input. Right Return all records from the right input. For each cross input match, return a record including the data from both sides. For any remaining non-matching records from the right input, return the left record along with nulls for the right input. Set Operation \u00b6 The set operation ecompasses several set level operations that support combining datasets based, possibly excluding records based on various types of record level matching. Signature Value Inputs 2 or more Outputs 1 Property Maintenance Maintains distribution if all inputs have the same ordinal distribution. Orderness is not maintained. Direct Output Order All inputs are ordinally matched and returned together. All inputs must have matching record types. Set Properties \u00b6 Property Description Required Primary Input The primary input of the dataset Required Secondary Inputs One or more relational inputs At least one required Set Operation Type From list below. Required Set Operation Types \u00b6 Property Description Minus (Primary) Returns the primary input excluding any matching records from secondary inputs. Minus (Multiset) Returns the primary input minus any records that are included in all sets. Intersection (Primary) Returns all rows primary rows that intersect at least one secondary input. Intersection (Multiset) Returns all rows that intersect at least one record from each secondary inputs. Union Distinct Returns all the records from each set, removing any rows that are duplicated (within or across sets). Union All Returns all records from each set, allowing duplicates. Fetch Operation \u00b6 The fetch operation eliminates records outside a desired window. Typically corresponds to a fetch/offset SQL clause. Will only returns records between the start offset and the end offset. Signature Value Inputs 2 or more Outputs 1 Property Maintenance Maintains distribution and orderedness. Direct Output Order Unchanged from input. Fetch Properties \u00b6 Property Description Required Input A relational input, typically with a desired orderedness property. Required Offset A positive integer. Declares the offset for retrieval of records. Optional, defaults to 0. Count A positive integer. Declares the number of records that should be returned. Required \u00b6 Aggregate Operation \u00b6 The aggregate operation groups input data on one or more sets of grouping keys, calculating each measure for each combination of grouping key. Signature Value Inputs 1 Outputs 1 Property Maintenance Maintains distribution if all distribution fields are contained in every grouping set. No orderness guaranteed. Direct Output Order The list of distinct columns from each grouping set (ordered by their first appearance) followed by the list of measures in declaration order, followed by a integer describing the associated particular grouping set the value is derived from. Aggregate Properties \u00b6 Property Description Required Input The relational input Required Grouping Sets One or more grouping sets Optional, required if no measures. Per Grouping Set A list of expression grouping that the aggregation measured should be calculated for. Optional, defaults to 0. Measures A list of one or more aggregate expressions. Optional, required if no grouping sets. \u00b6 Write Operator \u00b6 The write operator is an operator that consumes one output and writes it to storage. A simple example would be writing Parquet files. It is expected that many types of writes will be added over time Signature Value Inputs 1 Outputs 0 Property Maintenance N/A (no output) Direct Output Order N/A (no output) Write Properties \u00b6 Property Description Required Definition The contents of the write property definition. Required Field names The names of all struct fields in breadth-first order. Required Masked Complex Expression The masking expression applied to the input record prior to write. Optional, defaults to all Rotation description fields A list of fields that can be used for stream description whenever a stream is reset Optional, defaults to none. Rotation indiciator An input field id that describes when the current stream should be \u201crotated\u201d. Individual write definition types may support the ability to rotate the output into one or more streams. This could mean closing and opening a new file, finishing and restarting a TCP connection, etc. If a rotation indiciator is available, it will be 0 except when a rotation should occur. Rotation indication are frequently defined by things like discrete partition values but could be done based on number of records or other arbitrary criteria. Typically Optional, defaults to none. Write Definition Types \u00b6 Write definition types are built by the community and added to the specification. This is a portion of specification that is expected to grow rapidly. Virtual Table \u00b6 Property Description Required Name The in memory name to give the dataset. When used in tandem Required Pin Whether it is okay to remove this dataset from memory or it should be kept in memory. Optional, defaults to false. Files Type \u00b6 Property Description Required Path A uri to write the data to. Supports the inclusion of field references that are listed as available in properties as a \u201crotation description field\u201d. Required Format Enumeration of available formats. Only current option is PARQUET. Required Discussion Points \u00b6 How to handle correlated operations?","title":"Logical Relations"},{"location":"relations/logical_relations/#logical-relations","text":"","title":"Logical Relations"},{"location":"relations/logical_relations/#read-operator","text":"The read operator is an operator that produces one output. A simple example would be the reading of a Parquet file. It is expected that many types of reads will be added over time Signature Value Inputs 0 Outputs 1 Property Maintenance N/A (no inputs) Direct Output Order Defaults to the schema of the data read after the optional projection (masked complex expression) is applied.","title":"Read Operator"},{"location":"relations/logical_relations/#read-properties","text":"Property Description Required Definition The contents of the read property definition. Required Direct Schema Defines the schema of the output of the read (before any emit remapping/hiding). Required Filter A boolean Substrait expression that describes the filter of a iceberg dataset. TBD: define how field referencing works. Optional, defaults to none. Projection A masked complex expression describing the portions of the content that should be read Optional, defaults to all of schema Output properties Declaration of orderedness and/or distribution properties this read produces Optional, defaults to no properties. Properties A list of name/value pairs associated with the read Optional, defaults to empty","title":"Read Properties"},{"location":"relations/logical_relations/#read-definition-types","text":"Read definition types are built by the community and added to the specification. This is a portion of specification that is expected to grow rapidly.","title":"Read Definition Types"},{"location":"relations/logical_relations/#virtual-table","text":"Property Description Required Data Required Required","title":"Virtual Table"},{"location":"relations/logical_relations/#files-type","text":"Property Description Required Items An array Items (path or path glob) associated with the read Required Format per item Enumeration of available formats. Only current option is PARQUET. Required","title":"Files Type"},{"location":"relations/logical_relations/#filter-operation","text":"The filter operator eliminates one or more records from the input data based on a boolean filter expression. Signature Value Inputs 1 Outputs 1 Property Maintenance Orderedness, Distribution, remapped by emit Direct Output Order The field order as the input.","title":"Filter Operation"},{"location":"relations/logical_relations/#filter-properties","text":"Property Description Required Input The relational input Required Expression A boolean expression which describes which records are included/excluded. Required","title":"Filter Properties"},{"location":"relations/logical_relations/#sort-operation","text":"The sort operator reorders a dataset based on one or more identified sort fields as well as a sorting function. Signature Value Inputs 1 Outputs 1 Property Maintenance Will update orderedness property to the output of the sort operation. Distribution property only remapped based on emit. Direct Output Order The field order of the input.","title":"Sort Operation"},{"location":"relations/logical_relations/#sort-properties","text":"Property Description Required Input The relational input Required Sort Fields List of one or more fields to sort by. Uses the same properties as the orderedness property. One sort field required","title":"Sort Properties"},{"location":"relations/logical_relations/#project-operation","text":"The project operation will produce one or more additional expressions based on the inputs of the dataset. Signature Value Inputs 1 Outputs 1 Property Maintenance Distribution maintained, mapped by emit. Orderedness: Maintained if no window operations. Extended to include projection fields if fields are direct references. If window operations are present, no orderedness is maintained. Direct Output Order The field order of the input + the list of new expressions in the order they are declared in the expressions list.","title":"Project Operation"},{"location":"relations/logical_relations/#project-properties","text":"Property Description Required Input The relational input Required Expressions List of one or more expressions to add to the input. At least one expression required","title":"Project Properties"},{"location":"relations/logical_relations/#join-operation","text":"The join operation will combine two separate inputs into a single output, based on a join expression. A common subtype of joins is a equality join where the join expression is constrained to a list of equality (or equality + null equality) conditions between the two inputs of the join. Signature Value Inputs 2 Outputs 1 Property Maintenance Distribution is maintained. Orderedness is empty post operation. Physical relations may provide better property maintenance. Direct Output Order The emit order of the left input followed by the emit order of the right input.","title":"Join Operation"},{"location":"relations/logical_relations/#join-properties","text":"Property Description Required Left Input A relational input. Required Right Input A relational input. Required Join Expression A boolean condition that describes whether each record from the left set \u201cmatch\u201d the record from the right set. Field references correspond to the direct output order of the data. Required. Can be the literal True. Join Type One of the join types defined below. Required","title":"Join Properties"},{"location":"relations/logical_relations/#join-types","text":"Type Description Inner Return records from the left side only if they match the right side. Return records from the right side only when they match the left side. For each cross input match, return a record including the data from both sides. Non-matching records are ignored. Outer Return all records from both the left and right inputs. For each cross input match, return a record including the data from both sides. For any remaining non-match records, return the record from the corresponding input along with nulls for the opposite input. Left Return all records from the left input. For each cross input match, return a record including the data from both sides. For any remaining non-matching records from the left input, return the left record along with nulls for the right input. Right Return all records from the right input. For each cross input match, return a record including the data from both sides. For any remaining non-matching records from the right input, return the left record along with nulls for the right input.","title":"Join Types"},{"location":"relations/logical_relations/#set-operation","text":"The set operation ecompasses several set level operations that support combining datasets based, possibly excluding records based on various types of record level matching. Signature Value Inputs 2 or more Outputs 1 Property Maintenance Maintains distribution if all inputs have the same ordinal distribution. Orderness is not maintained. Direct Output Order All inputs are ordinally matched and returned together. All inputs must have matching record types.","title":"Set Operation"},{"location":"relations/logical_relations/#set-properties","text":"Property Description Required Primary Input The primary input of the dataset Required Secondary Inputs One or more relational inputs At least one required Set Operation Type From list below. Required","title":"Set Properties"},{"location":"relations/logical_relations/#set-operation-types","text":"Property Description Minus (Primary) Returns the primary input excluding any matching records from secondary inputs. Minus (Multiset) Returns the primary input minus any records that are included in all sets. Intersection (Primary) Returns all rows primary rows that intersect at least one secondary input. Intersection (Multiset) Returns all rows that intersect at least one record from each secondary inputs. Union Distinct Returns all the records from each set, removing any rows that are duplicated (within or across sets). Union All Returns all records from each set, allowing duplicates.","title":"Set Operation Types"},{"location":"relations/logical_relations/#fetch-operation","text":"The fetch operation eliminates records outside a desired window. Typically corresponds to a fetch/offset SQL clause. Will only returns records between the start offset and the end offset. Signature Value Inputs 2 or more Outputs 1 Property Maintenance Maintains distribution and orderedness. Direct Output Order Unchanged from input.","title":"Fetch Operation"},{"location":"relations/logical_relations/#fetch-properties","text":"Property Description Required Input A relational input, typically with a desired orderedness property. Required Offset A positive integer. Declares the offset for retrieval of records. Optional, defaults to 0. Count A positive integer. Declares the number of records that should be returned. Required","title":"Fetch Properties"},{"location":"relations/logical_relations/#_1","text":"","title":""},{"location":"relations/logical_relations/#aggregate-operation","text":"The aggregate operation groups input data on one or more sets of grouping keys, calculating each measure for each combination of grouping key. Signature Value Inputs 1 Outputs 1 Property Maintenance Maintains distribution if all distribution fields are contained in every grouping set. No orderness guaranteed. Direct Output Order The list of distinct columns from each grouping set (ordered by their first appearance) followed by the list of measures in declaration order, followed by a integer describing the associated particular grouping set the value is derived from.","title":"Aggregate Operation"},{"location":"relations/logical_relations/#aggregate-properties","text":"Property Description Required Input The relational input Required Grouping Sets One or more grouping sets Optional, required if no measures. Per Grouping Set A list of expression grouping that the aggregation measured should be calculated for. Optional, defaults to 0. Measures A list of one or more aggregate expressions. Optional, required if no grouping sets.","title":"Aggregate Properties"},{"location":"relations/logical_relations/#_2","text":"","title":""},{"location":"relations/logical_relations/#write-operator","text":"The write operator is an operator that consumes one output and writes it to storage. A simple example would be writing Parquet files. It is expected that many types of writes will be added over time Signature Value Inputs 1 Outputs 0 Property Maintenance N/A (no output) Direct Output Order N/A (no output)","title":"Write Operator"},{"location":"relations/logical_relations/#write-properties","text":"Property Description Required Definition The contents of the write property definition. Required Field names The names of all struct fields in breadth-first order. Required Masked Complex Expression The masking expression applied to the input record prior to write. Optional, defaults to all Rotation description fields A list of fields that can be used for stream description whenever a stream is reset Optional, defaults to none. Rotation indiciator An input field id that describes when the current stream should be \u201crotated\u201d. Individual write definition types may support the ability to rotate the output into one or more streams. This could mean closing and opening a new file, finishing and restarting a TCP connection, etc. If a rotation indiciator is available, it will be 0 except when a rotation should occur. Rotation indication are frequently defined by things like discrete partition values but could be done based on number of records or other arbitrary criteria. Typically Optional, defaults to none.","title":"Write Properties"},{"location":"relations/logical_relations/#write-definition-types","text":"Write definition types are built by the community and added to the specification. This is a portion of specification that is expected to grow rapidly.","title":"Write Definition Types"},{"location":"relations/logical_relations/#virtual-table_1","text":"Property Description Required Name The in memory name to give the dataset. When used in tandem Required Pin Whether it is okay to remove this dataset from memory or it should be kept in memory. Optional, defaults to false.","title":"Virtual Table"},{"location":"relations/logical_relations/#files-type_1","text":"Property Description Required Path A uri to write the data to. Supports the inclusion of field references that are listed as available in properties as a \u201crotation description field\u201d. Required Format Enumeration of available formats. Only current option is PARQUET. Required","title":"Files Type"},{"location":"relations/logical_relations/#discussion-points","text":"How to handle correlated operations?","title":"Discussion Points"},{"location":"relations/physical_relations/","text":"Physical Relations \u00b6 There is no true distinction between logical and physical operations in Substrait. By convention, certain operations are classified as physical but all operations can be potentially used in any kind of plan. A particular set of transformations or target operators may (by convention) be considered the \u201cphysical plan\u201d but this is a characteristic of the system consuming substrait as opposed to a definition within Substrait. Hash Equijoin Operator \u00b6 The hash equijoin join operator will build a hash table out of the right input based on a set of join keys. It will then probe that hash table for incoming inputs, finding matches. Signature Value Inputs 2 Outputs 1 Property Maintenance Distribution is maintained. Orderedness of the left set is maintained in INNER join cases, otherwise it is eliminated. Direct Output Order Same as the Join operator. Hash Equijoin Properties \u00b6 Property Description Required Left Input A relational input. Required Right Input A relational input. Required Join Expression A boolean condition that describes whether each record from the left set \u201cmatch\u201d the record from the right set. The condition must only include the following operations: AND, ==, field references, is not distinct from. Field references correspond to the direct output order of the data. Required. Post Join Predicate An additional expression that can be used to reduce the output of the join operation post the equality condition. Minimizes the overhead of secondary join conditions that cannot be evaluated using the equijoin keys. Optional, defaults true. Join Type One of the join types defined in the Join operator. Required NLJ Operator \u00b6 The nested loop join operator does a join by holding the entire right input and then iterating over it using the left input, evaluating the join expression on the cartesian product of all rows, only outputing rows where the expression is true. Will also include non-matching rows in the OUTER, LEFT and RIGHT operations per the join type requirements. Signature Value Inputs 2 Outputs 1 Property Maintenance Distribution is maintained. Orderedness is eliminated. Direct Output Order Same as the Join operator. NLJ Properties \u00b6 Property Description Required Left Input A relational input. Required Right Input A relational input. Required Join Expression A boolean condition that describes whether each record from the left set \u201cmatch\u201d the record from the right set. Optional. Defaults to true (a cartesian join). Join Type One of the join types defined in the Join operator. Required Merge Equijoin Operator \u00b6 The merge equijoin does a join by taking advantage of two sets that are sorted on the join keys. This allows the join operation to be done in a streaming fashion. Signature Value Inputs 2 Outputs 1 Property Maintenance Distribution is maintained. Orderedness is eliminated. Direct Output Order Same as the Join operator. Merge Join Properties \u00b6 Property Description Required Left Input A relational input. Required Right Input A relational input. Required Join Expression A boolean condition that describes whether each record from the left set \u201cmatch\u201d the record from the right set. The condition must only include the following operations: AND, ==, field references, is not distinct from. Field references correspond to the direct output order of the data. Optional. Defaults to tue (a cartesian join). Post Join Predicate An additional expression that can be used to reduce the output of the join operation post the equality condition. Minimizes the overhead of secondary join conditions that cannot be evaluated using the equijoin keys. Optional, defaults true. Join Type One of the join types defined in the Join operator. Required Distribute Operator \u00b6 The distribute operator will redistribute data based on zero or more distribution expressions. Applying this operation will lead to an output that presents the desired distribution. Signature Value Inputs 1 Outputs 1 Property Maintenance Orderedness is maintained. Distribution is overwritten based on configuration. Direct Output Order Order of the input. Distribute Properties \u00b6 Property Description Required Input The relational input Required. Expressions A list of expressions that describe how the data should be distributed. Optional. If undefined, data is expected to be distributed fairly evenly amonst destinations. Partition Count The number of partitions targeted for output Optional, defaults to the number of discrete values produced by the expressions. Expression Mapping A set of distribution expression tuples that are mapped to particular destinations. Optional, expressions of the same type are expected to be mapped to the destination given a consistent number of target partitions. Merging Capture \u00b6 A receiving operation that will merge multiple ordered streams to maintain orderedness. Signature Value Inputs 1 Outputs 1 Property Maintenance Orderedness and distribution are maintained. Direct Output Order Order of the input. Merging Capture Properties \u00b6 Property Description Required Blocking Whether the merging should block incoming data. Blocking should be used carefully, based on whether a deadlock can be produced. Optional, defaults to false Simple Capture \u00b6 A receiving operation that will merge multiple streams in an arbitrary order. Signature Value Inputs 1 Outputs 1 Property Maintenance Orderness is empty after this operation. Distribution are maintained. Direct Output Order Order of the input. Naive Capture Properties \u00b6 Property Description Required Input The relational input Required TopN Operation \u00b6 The topn operator reorders a dataset based on one or more identified sort fields as well as a sorting function. Rather than sort the entire dataset, the top-n will only maintain the total number of records required to ensure a limited output. A top-n is a combination of a logical sort and logical fetch operations. Signature Value Inputs 1 Outputs 1 Property Maintenance Will update orderedness property to the output of the sort operation. Distribution property only remapped based on emit. Direct Output Order The field order of the input. TopN Properties \u00b6 Property Description Required Input The relational input Required Sort Fields List of one or more fields to sort by. Uses the same properties as the orderedness property. One sort field required Offset A positive integer. Declares the offset for retrieval of records. Optional, defaults to 0. Count A positive integer. Declares the number of records that should be returned. Required Hash Aggregate Operation \u00b6 The hash aggregate operation maintains a hash table for each grouping set to collesce equivalent tuples. Signature Value Inputs 1 Outputs 1 Property Maintenance Maintains distribution if all distribution fields are contained in every grouping set. No orderness guaranteed. Direct Output Order Same as defined by Aggregate operation. Hash Aggregate Properties \u00b6 Property Description Required Input The relational input Required Grouping Sets One or more grouping sets Optional, required if no measures. Per Grouping Set A list of expression grouping that the aggregation measured should be calculated for. Optional, defaults to 0. Measures A list of one or more aggregate expressions. Implementations may or may not support aggregate ordering expressions. Optional, required if no grouping sets. Streaming Aggregate Operation \u00b6 The streaming aggregate operation leverages data ordered by the grouping expressions to calculate data each grouping set tuple-by-tuple in streaming fashion. All grouping sets and orderings requested on each aggregate must be compatible to allow multiple grouping sets or aggregate orderings. Signature Value Inputs 1 Outputs 1 Property Maintenance Maintains distribution if all distribution fields are contained in every grouping set. Maintains input ordering. Direct Output Order Same as defined by Aggregate operation. Streaming Aggregate Properties \u00b6 Property Description Required Input The relational input Required Grouping Sets One or more grouping sets. If multiple grouping sets are declared, sets must all be compatible with the the input sortedness. Optional, required if no measures. Per Grouping Set A list of expression grouping that the aggregation measured should be calculated for. Optional, defaults to 0. Measures A list of one or more aggregate expressions. Aggregate expressions ordering requirements must be compatible with expected ordering. Optional, required if no grouping sets. Hashing Window Operation \u00b6 A window aggregate operation that will build hash tables for each distinct partition expression. Signature Value Inputs 1 Outputs 1 Property Maintenance Maintains distribution. Eliminates ordering. Direct Output Order Same as Project operator (input followed by each window expression). Hashing Window Properties \u00b6 Property Description Required Input The relational input Required Window Expressions One or more window expressions At least one required. Streaming Window Operation \u00b6 A window aggregate operation that relies on a partition/ordering sorted input. Signature Value Inputs 1 Outputs 1 Property Maintenance Maintains distribution. Eliminates ordering. Direct Output Order Same as Project operator (input followed by each window expression). Streaming Window Properties \u00b6 Property Description Required Input The relational input Required Window Expressions One or more window expressions. Must be supported by the sorteness of the input. At least one required.","title":"Physical Relations"},{"location":"relations/physical_relations/#physical-relations","text":"There is no true distinction between logical and physical operations in Substrait. By convention, certain operations are classified as physical but all operations can be potentially used in any kind of plan. A particular set of transformations or target operators may (by convention) be considered the \u201cphysical plan\u201d but this is a characteristic of the system consuming substrait as opposed to a definition within Substrait.","title":"Physical Relations"},{"location":"relations/physical_relations/#hash-equijoin-operator","text":"The hash equijoin join operator will build a hash table out of the right input based on a set of join keys. It will then probe that hash table for incoming inputs, finding matches. Signature Value Inputs 2 Outputs 1 Property Maintenance Distribution is maintained. Orderedness of the left set is maintained in INNER join cases, otherwise it is eliminated. Direct Output Order Same as the Join operator.","title":"Hash Equijoin Operator"},{"location":"relations/physical_relations/#hash-equijoin-properties","text":"Property Description Required Left Input A relational input. Required Right Input A relational input. Required Join Expression A boolean condition that describes whether each record from the left set \u201cmatch\u201d the record from the right set. The condition must only include the following operations: AND, ==, field references, is not distinct from. Field references correspond to the direct output order of the data. Required. Post Join Predicate An additional expression that can be used to reduce the output of the join operation post the equality condition. Minimizes the overhead of secondary join conditions that cannot be evaluated using the equijoin keys. Optional, defaults true. Join Type One of the join types defined in the Join operator. Required","title":"Hash Equijoin Properties"},{"location":"relations/physical_relations/#nlj-operator","text":"The nested loop join operator does a join by holding the entire right input and then iterating over it using the left input, evaluating the join expression on the cartesian product of all rows, only outputing rows where the expression is true. Will also include non-matching rows in the OUTER, LEFT and RIGHT operations per the join type requirements. Signature Value Inputs 2 Outputs 1 Property Maintenance Distribution is maintained. Orderedness is eliminated. Direct Output Order Same as the Join operator.","title":"NLJ Operator"},{"location":"relations/physical_relations/#nlj-properties","text":"Property Description Required Left Input A relational input. Required Right Input A relational input. Required Join Expression A boolean condition that describes whether each record from the left set \u201cmatch\u201d the record from the right set. Optional. Defaults to true (a cartesian join). Join Type One of the join types defined in the Join operator. Required","title":"NLJ Properties"},{"location":"relations/physical_relations/#merge-equijoin-operator","text":"The merge equijoin does a join by taking advantage of two sets that are sorted on the join keys. This allows the join operation to be done in a streaming fashion. Signature Value Inputs 2 Outputs 1 Property Maintenance Distribution is maintained. Orderedness is eliminated. Direct Output Order Same as the Join operator.","title":"Merge Equijoin Operator"},{"location":"relations/physical_relations/#merge-join-properties","text":"Property Description Required Left Input A relational input. Required Right Input A relational input. Required Join Expression A boolean condition that describes whether each record from the left set \u201cmatch\u201d the record from the right set. The condition must only include the following operations: AND, ==, field references, is not distinct from. Field references correspond to the direct output order of the data. Optional. Defaults to tue (a cartesian join). Post Join Predicate An additional expression that can be used to reduce the output of the join operation post the equality condition. Minimizes the overhead of secondary join conditions that cannot be evaluated using the equijoin keys. Optional, defaults true. Join Type One of the join types defined in the Join operator. Required","title":"Merge Join Properties"},{"location":"relations/physical_relations/#distribute-operator","text":"The distribute operator will redistribute data based on zero or more distribution expressions. Applying this operation will lead to an output that presents the desired distribution. Signature Value Inputs 1 Outputs 1 Property Maintenance Orderedness is maintained. Distribution is overwritten based on configuration. Direct Output Order Order of the input.","title":"Distribute Operator"},{"location":"relations/physical_relations/#distribute-properties","text":"Property Description Required Input The relational input Required. Expressions A list of expressions that describe how the data should be distributed. Optional. If undefined, data is expected to be distributed fairly evenly amonst destinations. Partition Count The number of partitions targeted for output Optional, defaults to the number of discrete values produced by the expressions. Expression Mapping A set of distribution expression tuples that are mapped to particular destinations. Optional, expressions of the same type are expected to be mapped to the destination given a consistent number of target partitions.","title":"Distribute Properties"},{"location":"relations/physical_relations/#merging-capture","text":"A receiving operation that will merge multiple ordered streams to maintain orderedness. Signature Value Inputs 1 Outputs 1 Property Maintenance Orderedness and distribution are maintained. Direct Output Order Order of the input.","title":"Merging Capture"},{"location":"relations/physical_relations/#merging-capture-properties","text":"Property Description Required Blocking Whether the merging should block incoming data. Blocking should be used carefully, based on whether a deadlock can be produced. Optional, defaults to false","title":"Merging Capture Properties"},{"location":"relations/physical_relations/#simple-capture","text":"A receiving operation that will merge multiple streams in an arbitrary order. Signature Value Inputs 1 Outputs 1 Property Maintenance Orderness is empty after this operation. Distribution are maintained. Direct Output Order Order of the input.","title":"Simple Capture"},{"location":"relations/physical_relations/#naive-capture-properties","text":"Property Description Required Input The relational input Required","title":"Naive Capture Properties"},{"location":"relations/physical_relations/#topn-operation","text":"The topn operator reorders a dataset based on one or more identified sort fields as well as a sorting function. Rather than sort the entire dataset, the top-n will only maintain the total number of records required to ensure a limited output. A top-n is a combination of a logical sort and logical fetch operations. Signature Value Inputs 1 Outputs 1 Property Maintenance Will update orderedness property to the output of the sort operation. Distribution property only remapped based on emit. Direct Output Order The field order of the input.","title":"TopN Operation"},{"location":"relations/physical_relations/#topn-properties","text":"Property Description Required Input The relational input Required Sort Fields List of one or more fields to sort by. Uses the same properties as the orderedness property. One sort field required Offset A positive integer. Declares the offset for retrieval of records. Optional, defaults to 0. Count A positive integer. Declares the number of records that should be returned. Required","title":"TopN Properties"},{"location":"relations/physical_relations/#hash-aggregate-operation","text":"The hash aggregate operation maintains a hash table for each grouping set to collesce equivalent tuples. Signature Value Inputs 1 Outputs 1 Property Maintenance Maintains distribution if all distribution fields are contained in every grouping set. No orderness guaranteed. Direct Output Order Same as defined by Aggregate operation.","title":"Hash Aggregate Operation"},{"location":"relations/physical_relations/#hash-aggregate-properties","text":"Property Description Required Input The relational input Required Grouping Sets One or more grouping sets Optional, required if no measures. Per Grouping Set A list of expression grouping that the aggregation measured should be calculated for. Optional, defaults to 0. Measures A list of one or more aggregate expressions. Implementations may or may not support aggregate ordering expressions. Optional, required if no grouping sets.","title":"Hash Aggregate Properties"},{"location":"relations/physical_relations/#streaming-aggregate-operation","text":"The streaming aggregate operation leverages data ordered by the grouping expressions to calculate data each grouping set tuple-by-tuple in streaming fashion. All grouping sets and orderings requested on each aggregate must be compatible to allow multiple grouping sets or aggregate orderings. Signature Value Inputs 1 Outputs 1 Property Maintenance Maintains distribution if all distribution fields are contained in every grouping set. Maintains input ordering. Direct Output Order Same as defined by Aggregate operation.","title":"Streaming Aggregate Operation"},{"location":"relations/physical_relations/#streaming-aggregate-properties","text":"Property Description Required Input The relational input Required Grouping Sets One or more grouping sets. If multiple grouping sets are declared, sets must all be compatible with the the input sortedness. Optional, required if no measures. Per Grouping Set A list of expression grouping that the aggregation measured should be calculated for. Optional, defaults to 0. Measures A list of one or more aggregate expressions. Aggregate expressions ordering requirements must be compatible with expected ordering. Optional, required if no grouping sets.","title":"Streaming Aggregate Properties"},{"location":"relations/physical_relations/#hashing-window-operation","text":"A window aggregate operation that will build hash tables for each distinct partition expression. Signature Value Inputs 1 Outputs 1 Property Maintenance Maintains distribution. Eliminates ordering. Direct Output Order Same as Project operator (input followed by each window expression).","title":"Hashing Window Operation"},{"location":"relations/physical_relations/#hashing-window-properties","text":"Property Description Required Input The relational input Required Window Expressions One or more window expressions At least one required.","title":"Hashing Window Properties"},{"location":"relations/physical_relations/#streaming-window-operation","text":"A window aggregate operation that relies on a partition/ordering sorted input. Signature Value Inputs 1 Outputs 1 Property Maintenance Maintains distribution. Eliminates ordering. Direct Output Order Same as Project operator (input followed by each window expression).","title":"Streaming Window Operation"},{"location":"relations/physical_relations/#streaming-window-properties","text":"Property Description Required Input The relational input Required Window Expressions One or more window expressions. Must be supported by the sorteness of the input. At least one required.","title":"Streaming Window Properties"},{"location":"relations/user_defined_relations/","text":"User Defined Relations \u00b6 Pending","title":"User Defined Relations"},{"location":"relations/user_defined_relations/#user-defined-relations","text":"Pending","title":"User Defined Relations"},{"location":"serialization/binary_serialization/","text":"Binary Serialization \u00b6 The binary format of a Substrait is designed to be easy to work with in many languages. A key requirement is that someone can take the binary format IDL and use standard tools to build a set of primitives that are easy to work with in any of a number of languages. This allows communities to build and use Substrait using only a binary IDL and the specification (and allows the Substrait project to avoid being required to build libraries for each language to work with the specification). There are several binary IDLs that exist today. The key requirements for Substrait are the following: Strongly typed IDL schema language High Quality well supported and idiomatic bindings/compilers for key languages (Python, Javascript, C++, Go, Rust, Java) Compact serial representation The primary formats that exist that roughly qualify under these requirements include: Protobuf, Thrift, Flatbuf, Avro, Cap\u2019N\u2019Proto. The current plan is to use Protobuf, primarily due to its clean typing system and large number of high quality language bindings. Flatbuf is a close second but it\u2019s poor support for unions along with the complexity of api use in many languages make it unsuitable to a project that is trying to avoid having to generate per language bindings to grow initial adoption of the core plan specification. The binary serialization representation is being developed within GitHub and is planned to be presented inline in the spec to help clarify the relationship between the specification and the serialized representations.","title":"Binary Serialization"},{"location":"serialization/binary_serialization/#binary-serialization","text":"The binary format of a Substrait is designed to be easy to work with in many languages. A key requirement is that someone can take the binary format IDL and use standard tools to build a set of primitives that are easy to work with in any of a number of languages. This allows communities to build and use Substrait using only a binary IDL and the specification (and allows the Substrait project to avoid being required to build libraries for each language to work with the specification). There are several binary IDLs that exist today. The key requirements for Substrait are the following: Strongly typed IDL schema language High Quality well supported and idiomatic bindings/compilers for key languages (Python, Javascript, C++, Go, Rust, Java) Compact serial representation The primary formats that exist that roughly qualify under these requirements include: Protobuf, Thrift, Flatbuf, Avro, Cap\u2019N\u2019Proto. The current plan is to use Protobuf, primarily due to its clean typing system and large number of high quality language bindings. Flatbuf is a close second but it\u2019s poor support for unions along with the complexity of api use in many languages make it unsuitable to a project that is trying to avoid having to generate per language bindings to grow initial adoption of the core plan specification. The binary serialization representation is being developed within GitHub and is planned to be presented inline in the spec to help clarify the relationship between the specification and the serialized representations.","title":"Binary Serialization"},{"location":"serialization/text_serialization/","text":"Text Serialization \u00b6 To maximize the new user experience, it is important for Substrait to have a text representation of plans. This allows people to experiment with basic tooling. Building simple CLI tools that do things like SQL > Plan and Plan > SQL or REPL plan construction can all be done relatively straightforwardly with a text representation. The recommended text serialization format is JSON. Since the text format is not designed for performance, the format can be produced to maximize readability. This also allows nice symmetry between the construction of plans and the configuration of various extensions such as function signatures and user defined types. To ensure the JSON is valid, the object will defined using the OpenApi 3.1 specification . This not only allows strong validation, the openapi specification enables code generators to be easily used to produce plans in many languages. While JSON will be used for much of the plan serialization, Substrait uses a custom simplistic grammar for record level expressions. While one can construct an equation such as (10 + 5)/2 using a tree of function and literal objects, it is much more human readable to consume a plan when the information is written similarly to the way one typically consumes scalar expressions. This grammar will be maintained in an ANTLR grammar (targetable to multiple programming languages) and is also planned to be supported via JSON schema definition format tag so that the grammar can be validated as part of the schema validation.","title":"Text Serialization"},{"location":"serialization/text_serialization/#text-serialization","text":"To maximize the new user experience, it is important for Substrait to have a text representation of plans. This allows people to experiment with basic tooling. Building simple CLI tools that do things like SQL > Plan and Plan > SQL or REPL plan construction can all be done relatively straightforwardly with a text representation. The recommended text serialization format is JSON. Since the text format is not designed for performance, the format can be produced to maximize readability. This also allows nice symmetry between the construction of plans and the configuration of various extensions such as function signatures and user defined types. To ensure the JSON is valid, the object will defined using the OpenApi 3.1 specification . This not only allows strong validation, the openapi specification enables code generators to be easily used to produce plans in many languages. While JSON will be used for much of the plan serialization, Substrait uses a custom simplistic grammar for record level expressions. While one can construct an equation such as (10 + 5)/2 using a tree of function and literal objects, it is much more human readable to consume a plan when the information is written similarly to the way one typically consumes scalar expressions. This grammar will be maintained in an ANTLR grammar (targetable to multiple programming languages) and is also planned to be supported via JSON schema definition format tag so that the grammar can be validated as part of the schema validation.","title":"Text Serialization"},{"location":"spec/specification/","text":"Specification \u00b6 Process \u00b6 The goal of this project is initially to establish a well-defined specification. Once established, new versions of the specification will follow a normal development/release process. To provide something to peruse while clarifying an openness to the community during the initial development of the specification, we plan to follow the following steps for development of the specification. We will use github branches to describe each of these steps and patches will be proposed to be moved from one branch to the next to allow review of documents while still having strawmen to start with. The steps are: Empty - No outline has been produced. A sketch needs to be produced for people to react and iterate on. Sketch - Something has been written but should serve more as a conceptual backing for what should be achieved in this part of the specification. No collaboration or consensus has occurred. This will be discussed and iterated on until an initial WIP version can be patched. The WIP version will be held in a PR to iterate on until it is committed to the WIP branch of the repository. WIP - An initial version that multiple contributors have agreed to has been produced for this portion of the specification. Any user is welcome to propose additional changes or discussions with regards to this component but it now represents a community intention. Commit - Believed to be a well-formed plan for this portion of the specification. Documents that have had no outstanding reviews for 14 days will be moved from WIP to commit. Changes can still be made but the section should no longer be under constant revision. (This status is more for external observers to understand the progress of the specification than something that influences internal project process.) Once all portions of the specification have been moved to commit (or eliminated), the specification will move to an initial version number. To try to get a working end-to-end model as quickly as possible, a small number of items have been prioritized. The set of components outlined here are proposed as a mechanism for having bite-size review/discussion chunks to make forward progress. Components \u00b6 Priority Status Section Description 1 wip Simple Types A way to describe the set of basic types that will be operated on within a plan. Only includes simple types such as integers and doubles (nothing configurable or compound). wip Compound Types Expression of types that go beyond simple scalar values. Key concepts here include: configurable types such as fixed length and numeric types as well as compound types such as structs, maps, lists, etc. wip Type Variations Physical variations to base types. sketch User Defined Types Extensions that can be defined for specific IR producers/consumers. 2 sketch Field References Expressions to identify which portions of a record should be 3 sketch Scalar Functions Description of how functions are specified. Concepts include arguments, variadic functions, output type derivation, etc. sketch Scalar Function List A list of well-known canonical functions in yaml format. sketch Specialized Record Expressions Specialized expression types that are more naturally expressed outside the function paradigm. Examples include items such as if/then/else and switch statements. sketch Aggregate Functions Functions that are expressed in aggregation operations. Examples include things such as SUM, COUNT, etc. Operations take many records and collapse them into a single (possibly compound) value. sketch Window Functions Functions that relate a record to a set of encompassing records. Examples in SQL include RANK, NTILE, etc. empty Table Functions Functions that convert one or more values from an input record into 0..N output records. Example include operations such as explode, pos-exlode, etc. sketch User Defined Functions Reusable named functions that are built beyond the core specification. Implementations are typically registered thorugh external means (drop a file in a directory, send a special command with implementation, etc). sketch Embedded Functions Functions implementations embedded directly within the plan. Frequently used in data scicence workflows where business logic is interpersed with standard operations. 4 sketch Relation Basics Basic concepts around relational algebra, record emit and properties. sketch Logical Relations Common relational operations used in compute plans including project, join, aggregation, etc. sketch Physical Relations Specific execution sub-variations of common relational operations that describe have multiple unique physical variants associated with a single logical operation. Examples include hash join, merge join, nested loop join, etc. empty User Defined Relations Installed and reusable relational operations customized to a particular platform. empty Embedded Relations Relational operations where plans contain the \u201cmachine code\u201d to directly execute the necessary operations. 5 sketch Text Serialization A human producable & consumable representation of the plan specification. 6 sketch Binary Serialization A high performance & compact binary representation of the plan specification.","title":"Specification"},{"location":"spec/specification/#specification","text":"","title":"Specification"},{"location":"spec/specification/#process","text":"The goal of this project is initially to establish a well-defined specification. Once established, new versions of the specification will follow a normal development/release process. To provide something to peruse while clarifying an openness to the community during the initial development of the specification, we plan to follow the following steps for development of the specification. We will use github branches to describe each of these steps and patches will be proposed to be moved from one branch to the next to allow review of documents while still having strawmen to start with. The steps are: Empty - No outline has been produced. A sketch needs to be produced for people to react and iterate on. Sketch - Something has been written but should serve more as a conceptual backing for what should be achieved in this part of the specification. No collaboration or consensus has occurred. This will be discussed and iterated on until an initial WIP version can be patched. The WIP version will be held in a PR to iterate on until it is committed to the WIP branch of the repository. WIP - An initial version that multiple contributors have agreed to has been produced for this portion of the specification. Any user is welcome to propose additional changes or discussions with regards to this component but it now represents a community intention. Commit - Believed to be a well-formed plan for this portion of the specification. Documents that have had no outstanding reviews for 14 days will be moved from WIP to commit. Changes can still be made but the section should no longer be under constant revision. (This status is more for external observers to understand the progress of the specification than something that influences internal project process.) Once all portions of the specification have been moved to commit (or eliminated), the specification will move to an initial version number. To try to get a working end-to-end model as quickly as possible, a small number of items have been prioritized. The set of components outlined here are proposed as a mechanism for having bite-size review/discussion chunks to make forward progress.","title":"Process"},{"location":"spec/specification/#components","text":"Priority Status Section Description 1 wip Simple Types A way to describe the set of basic types that will be operated on within a plan. Only includes simple types such as integers and doubles (nothing configurable or compound). wip Compound Types Expression of types that go beyond simple scalar values. Key concepts here include: configurable types such as fixed length and numeric types as well as compound types such as structs, maps, lists, etc. wip Type Variations Physical variations to base types. sketch User Defined Types Extensions that can be defined for specific IR producers/consumers. 2 sketch Field References Expressions to identify which portions of a record should be 3 sketch Scalar Functions Description of how functions are specified. Concepts include arguments, variadic functions, output type derivation, etc. sketch Scalar Function List A list of well-known canonical functions in yaml format. sketch Specialized Record Expressions Specialized expression types that are more naturally expressed outside the function paradigm. Examples include items such as if/then/else and switch statements. sketch Aggregate Functions Functions that are expressed in aggregation operations. Examples include things such as SUM, COUNT, etc. Operations take many records and collapse them into a single (possibly compound) value. sketch Window Functions Functions that relate a record to a set of encompassing records. Examples in SQL include RANK, NTILE, etc. empty Table Functions Functions that convert one or more values from an input record into 0..N output records. Example include operations such as explode, pos-exlode, etc. sketch User Defined Functions Reusable named functions that are built beyond the core specification. Implementations are typically registered thorugh external means (drop a file in a directory, send a special command with implementation, etc). sketch Embedded Functions Functions implementations embedded directly within the plan. Frequently used in data scicence workflows where business logic is interpersed with standard operations. 4 sketch Relation Basics Basic concepts around relational algebra, record emit and properties. sketch Logical Relations Common relational operations used in compute plans including project, join, aggregation, etc. sketch Physical Relations Specific execution sub-variations of common relational operations that describe have multiple unique physical variants associated with a single logical operation. Examples include hash join, merge join, nested loop join, etc. empty User Defined Relations Installed and reusable relational operations customized to a particular platform. empty Embedded Relations Relational operations where plans contain the \u201cmachine code\u201d to directly execute the necessary operations. 5 sketch Text Serialization A human producable & consumable representation of the plan specification. 6 sketch Binary Serialization A high performance & compact binary representation of the plan specification.","title":"Components"},{"location":"spec/technology_principles/","text":"Technology Principles \u00b6 Provide a good suite of well-specified common functionality in databases and data science applications. Make it easy for users to privately or publically extend the representation to support specialized/custom operations. Produce something that is language agnostic and requires minimal work to start developing against in a new language. Drive towards a common format that avoids specialization for single favorite producer or consumer. Establish clear delineation between specifications that MUST be respected to and those that can be optionally ignored. Establish a forgiving compatibility approach and versioning scheme that supports cross-version compatibility in maximum number of cases. Minimize the need for consumer intelligence by excluding concepts like overloading, type coercion, implicit casting, field name handling, etc. (Note: this is weak and should be better stated.) Decomposability/severability: A particular producer or consumer should be able to produce or consume only a subset of the specification and interact well with any other Substrait system as long the specific operations requested fit within the subset of specification supported by the counter system.","title":"Technology Principles"},{"location":"spec/technology_principles/#technology-principles","text":"Provide a good suite of well-specified common functionality in databases and data science applications. Make it easy for users to privately or publically extend the representation to support specialized/custom operations. Produce something that is language agnostic and requires minimal work to start developing against in a new language. Drive towards a common format that avoids specialization for single favorite producer or consumer. Establish clear delineation between specifications that MUST be respected to and those that can be optionally ignored. Establish a forgiving compatibility approach and versioning scheme that supports cross-version compatibility in maximum number of cases. Minimize the need for consumer intelligence by excluding concepts like overloading, type coercion, implicit casting, field name handling, etc. (Note: this is weak and should be better stated.) Decomposability/severability: A particular producer or consumer should be able to produce or consume only a subset of the specification and interact well with any other Substrait system as long the specific operations requested fit within the subset of specification supported by the counter system.","title":"Technology Principles"},{"location":"types/compound_logical_types/","text":"Compound Types \u00b6 Compound types include any type that is configurable including complex types as well as configurable scalar types. Type Name Description Arrow Analog Iceberg Analog Spark Analog Trino Analog FIXEDCHAR(L) A fixed length field of length L. L can be between [1..2,147,483,647]. Values less that are less in length than the length of the field are padded with spaces. None None CharType(L) CHAR(L) VARCHAR(L) A field that can holds UTF8 encoded strings between 0 and L length. The length of each value can be between [0..2,147,483,647]. The value of L can be between [1..2,147,483,647]. Values shorter than L are not padded. None None VarcharType(L) VARCHAR(L) FIXEDBINARY(L) A binary field that is fixed in width to L. Values that are shorter than L are 0-byte padded. FixedSizeBinary<L> FIXED<L> - - DECIMAL(P,S) A fixed precision decimal value having precision (P, number of digits) <= 38 and Scale (S, number of fractional digits) 0 <= S <= P) Decimal<P, S, bitwidth=128> DECIMAL(P,S) DECIMAL(P,S) DECIMAL(P,S) STRUCT<N:T1,\u2026,N:T2> A struct that maps unique names to value types. Each name is a UTF8 string. Each value can have a distinct type. struct_<*> struct<*> struct<*> row<*> LIST<T> A list of values of type T. The list can be between [0..2,147,483,647] values in length. list list list array MAP<K, V> An unordered list of type K keys with type V values. map<k,v> map<k,v> - map<k,v>","title":"Compound Types"},{"location":"types/compound_logical_types/#compound-types","text":"Compound types include any type that is configurable including complex types as well as configurable scalar types. Type Name Description Arrow Analog Iceberg Analog Spark Analog Trino Analog FIXEDCHAR(L) A fixed length field of length L. L can be between [1..2,147,483,647]. Values less that are less in length than the length of the field are padded with spaces. None None CharType(L) CHAR(L) VARCHAR(L) A field that can holds UTF8 encoded strings between 0 and L length. The length of each value can be between [0..2,147,483,647]. The value of L can be between [1..2,147,483,647]. Values shorter than L are not padded. None None VarcharType(L) VARCHAR(L) FIXEDBINARY(L) A binary field that is fixed in width to L. Values that are shorter than L are 0-byte padded. FixedSizeBinary<L> FIXED<L> - - DECIMAL(P,S) A fixed precision decimal value having precision (P, number of digits) <= 38 and Scale (S, number of fractional digits) 0 <= S <= P) Decimal<P, S, bitwidth=128> DECIMAL(P,S) DECIMAL(P,S) DECIMAL(P,S) STRUCT<N:T1,\u2026,N:T2> A struct that maps unique names to value types. Each name is a UTF8 string. Each value can have a distinct type. struct_<*> struct<*> struct<*> row<*> LIST<T> A list of values of type T. The list can be between [0..2,147,483,647] values in length. list list list array MAP<K, V> An unordered list of type K keys with type V values. map<k,v> map<k,v> - map<k,v>","title":"Compound Types"},{"location":"types/simple_logical_types/","text":"Simple Types \u00b6 Substrait tries to cover the most common types used in data manipulation. Simple types are those that don\u2019t support any form of configuration. For simplicity, any generic type that has only a small number of discrete implementations is declared directly (as opposed to via configuration). To minimize type explosion, the project currently follows the guideline that a logical type should probably only be included in the specification if it is included in at least two of the following open source Projects: Apache Arrow, Apache Iceberg, Apache Spark and Trino. Type Name Description Arrow Analog Iceberg Analog Spark Analog Trino Analog boolean A value that is either True or False. Bool boolean boolean boolean i8 A signed 8 byte value in [-128..127] Int<8,true> - ByteType tinyint i16 A signed 16 byte value between [-32,768..32,767] Int<16,true> - ShortType smallint i32 A signed 32 byte value between [-2147483648..2,147,483,647] Int<32,true> int IntegerType int i64 A signed 64 byte value between [\u22129,223,372,036,854,775,808..9,223,372,036,854,775,807] Int<64,true> long LongType bigint fp32 A 4 byte single precision floating point number with range as defined here . Float<SINGLE> float FloatType real fp64 An 8 byte double precision floating point number with range as defined here . Float<DOUBLE> double DecimalType double string A string of text. [0..2,147,483,647] bytes in length. String is encoded using UTF8 encoding. Utf8 string StringType varchar (no len) binary A binary value. [0..2,147,483,647] bytes in length. Binary binary BinaryType Varbinary timestamp A naive timestamp with microsecond precision that cannot be mapped to a moment on the timeline. Similar to naive datetime in Python. Supports range of [1000-01-01 00:00:00.000000..9999-12-31 23:59:59.999999] timestamp<MICROSECOND> timestamp TimestampType timestamp(6) timestamp_tz A timestamp with microseconds precision that is mapped to an instant in time. Similar to aware datetime in Python. Supports a range of [1000-01-01 00:00:00.000000..9999-12-31 23:59:59.999999] UTC. timestamp<micro;utc> timestamptz - timestamp(6) with time zone date A date. Range of [1000-01-01..9999-12-31]. Date<MILLISECOND> date DateType Date time A time with microsecond precision since the beginning of any day. Range of [0..86,399,999,999] microseconds. Time<MICROSECOND;64> time time(6) time(6) interval_year Interval year to month. Supports a range of any combination of years and months that total less than or equal to 10,000 years. Each component can be specified as positive or negative. Examples minimums/maximums include: [10000y, -120000m, 1y119988m, 1000y108000m, etc]. Note that each component can never independently specify more than 10,000 years, (even if the components have opposite signs e.g. -10000y200000m is not allowed). INTERVAL<YEAR_MONTH> - - Interval year to month interval_day Interval day to second with microsecond precision. Supports a range of [-3,650,000..3,650,000] days and [-9,223,372,036,854,775..9,223,372,036,854,775] microseconds in any combination. INTERVAL<MONTH_DAY_NANO> - - Interval day to second uuid A universally unique identifier composed of 128bits. Typically presented to users in hexadecimal format such as: c48ffa9e-64f4-44cb-ae47-152b4e60e77b . Any 128 bit value is allowed without specific adherance to RFC4122. uuid UUID","title":"Simple Types"},{"location":"types/simple_logical_types/#simple-types","text":"Substrait tries to cover the most common types used in data manipulation. Simple types are those that don\u2019t support any form of configuration. For simplicity, any generic type that has only a small number of discrete implementations is declared directly (as opposed to via configuration). To minimize type explosion, the project currently follows the guideline that a logical type should probably only be included in the specification if it is included in at least two of the following open source Projects: Apache Arrow, Apache Iceberg, Apache Spark and Trino. Type Name Description Arrow Analog Iceberg Analog Spark Analog Trino Analog boolean A value that is either True or False. Bool boolean boolean boolean i8 A signed 8 byte value in [-128..127] Int<8,true> - ByteType tinyint i16 A signed 16 byte value between [-32,768..32,767] Int<16,true> - ShortType smallint i32 A signed 32 byte value between [-2147483648..2,147,483,647] Int<32,true> int IntegerType int i64 A signed 64 byte value between [\u22129,223,372,036,854,775,808..9,223,372,036,854,775,807] Int<64,true> long LongType bigint fp32 A 4 byte single precision floating point number with range as defined here . Float<SINGLE> float FloatType real fp64 An 8 byte double precision floating point number with range as defined here . Float<DOUBLE> double DecimalType double string A string of text. [0..2,147,483,647] bytes in length. String is encoded using UTF8 encoding. Utf8 string StringType varchar (no len) binary A binary value. [0..2,147,483,647] bytes in length. Binary binary BinaryType Varbinary timestamp A naive timestamp with microsecond precision that cannot be mapped to a moment on the timeline. Similar to naive datetime in Python. Supports range of [1000-01-01 00:00:00.000000..9999-12-31 23:59:59.999999] timestamp<MICROSECOND> timestamp TimestampType timestamp(6) timestamp_tz A timestamp with microseconds precision that is mapped to an instant in time. Similar to aware datetime in Python. Supports a range of [1000-01-01 00:00:00.000000..9999-12-31 23:59:59.999999] UTC. timestamp<micro;utc> timestamptz - timestamp(6) with time zone date A date. Range of [1000-01-01..9999-12-31]. Date<MILLISECOND> date DateType Date time A time with microsecond precision since the beginning of any day. Range of [0..86,399,999,999] microseconds. Time<MICROSECOND;64> time time(6) time(6) interval_year Interval year to month. Supports a range of any combination of years and months that total less than or equal to 10,000 years. Each component can be specified as positive or negative. Examples minimums/maximums include: [10000y, -120000m, 1y119988m, 1000y108000m, etc]. Note that each component can never independently specify more than 10,000 years, (even if the components have opposite signs e.g. -10000y200000m is not allowed). INTERVAL<YEAR_MONTH> - - Interval year to month interval_day Interval day to second with microsecond precision. Supports a range of [-3,650,000..3,650,000] days and [-9,223,372,036,854,775..9,223,372,036,854,775] microseconds in any combination. INTERVAL<MONTH_DAY_NANO> - - Interval day to second uuid A universally unique identifier composed of 128bits. Typically presented to users in hexadecimal format such as: c48ffa9e-64f4-44cb-ae47-152b4e60e77b . Any 128 bit value is allowed without specific adherance to RFC4122. uuid UUID","title":"Simple Types"},{"location":"types/type_variations/","text":"Type Variations \u00b6 Since Substrait is designed to work in both logical and physical contexts, there is a need to support extended attributes in the physical context. Different consumers may have multiple ways to present the same logical type. For example, an engine might support dictionary encoding a string or using either a row-wise or columnar representation of a struct. As such, there is the facility for specification users to express additional type variations for each logical type. These variations are expected to have the same logical properties as the canonical variation and are defined for each organization. The key properties of these variations are: Property Description Base Type The base type this variation belongs to. Variations can only be expressed for simple types and wild-carded compound types (e.g. i8 or varchar(*)). Name The name used to reference this type. Should be unique within type variations for this parent type within an organization. Description A human description of the purpose of this type variation Function Behavior Inherits or Independent : Whether this variation supports functions using the canonical variation or whether functions should be resolved independently. For example if one has the function add(i8,i8) defined and then defines an i8 variation, can the i8 variation field be bound to the base add operation (inherits) or does a specialized version of add need to be defined specifically for this type variation (independent). Defaults to inherits.","title":"Type Variations"},{"location":"types/type_variations/#type-variations","text":"Since Substrait is designed to work in both logical and physical contexts, there is a need to support extended attributes in the physical context. Different consumers may have multiple ways to present the same logical type. For example, an engine might support dictionary encoding a string or using either a row-wise or columnar representation of a struct. As such, there is the facility for specification users to express additional type variations for each logical type. These variations are expected to have the same logical properties as the canonical variation and are defined for each organization. The key properties of these variations are: Property Description Base Type The base type this variation belongs to. Variations can only be expressed for simple types and wild-carded compound types (e.g. i8 or varchar(*)). Name The name used to reference this type. Should be unique within type variations for this parent type within an organization. Description A human description of the purpose of this type variation Function Behavior Inherits or Independent : Whether this variation supports functions using the canonical variation or whether functions should be resolved independently. For example if one has the function add(i8,i8) defined and then defines an i8 variation, can the i8 variation field be bound to the base add operation (inherits) or does a specialized version of add need to be defined specifically for this type variation (independent). Defaults to inherits.","title":"Type Variations"},{"location":"types/user_defined_types/","text":"User Defined Types \u00b6 User Defined Types can be created using a combination of pre-defined simple and compound types. User defined types are defined as an extension and are classified by the organization that they belong. An organization can declare an arbitrary number of user defined extension types. Initially, user defined types must be simple types (although they can be constructed of a number of inner compound and simple types). A yaml example of an extension type is below: name : point structure : longitude : i32 latitude : i32 This declares a new type (namespaced to the associated organization) called \u201cpoint\u201d. This type is composed of two i32 values named longitude and latitude. Once a type has been declared, it can be used in function declarations. [TBD: should field references be allowed to dereference the components of a user defined type?]","title":"User Defined Types"},{"location":"types/user_defined_types/#user-defined-types","text":"User Defined Types can be created using a combination of pre-defined simple and compound types. User defined types are defined as an extension and are classified by the organization that they belong. An organization can declare an arbitrary number of user defined extension types. Initially, user defined types must be simple types (although they can be constructed of a number of inner compound and simple types). A yaml example of an extension type is below: name : point structure : longitude : i32 latitude : i32 This declares a new type (namespaced to the associated organization) called \u201cpoint\u201d. This type is composed of two i32 values named longitude and latitude. Once a type has been declared, it can be used in function declarations. [TBD: should field references be allowed to dereference the components of a user defined type?]","title":"User Defined Types"}]}